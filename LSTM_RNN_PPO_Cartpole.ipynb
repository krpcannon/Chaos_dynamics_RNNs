{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b7d19aa-f3a1-429f-ae41-81901a245da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPO with RNN - LSTM\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from gym import *\n",
    "import scipy.signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f10fd6-a3c3-4edc-85c7-b3cf7cb638d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOW PPO Works now (Keras example)\n",
    "\n",
    "#\n",
    "#def mlp(x, sizes, activation=keras.activations.tanh, output_activation=None):\n",
    "    # Build a feedforward neural network\n",
    "#    for size in sizes[:-1]:\n",
    "#        x = layers.Dense(units=size, activation=activation)(x)\n",
    "\n",
    "#    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "#observation_input = keras.Input(shape=(observation_dimensions,), dtype=\"float32\")\n",
    "\n",
    "\n",
    "#logits = mlp(observation_input, list(wandb.config.get('hidden_sizes')) + [num_actions])\n",
    "\n",
    "    \n",
    "#actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "\n",
    "#value = keras.ops.squeeze(mlp(observation_input, list(wandb.config.get('hidden_sizes')) + [1]), axis=1)\n",
    "#critic = keras.Model(inputs=observation_input, outputs=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60285202-4a19-41f2-84e2-661e973ff2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n",
    "\n",
    "\n",
    "#def mlp(x, sizes, activation=keras.activations.tanh, output_activation=None):\n",
    "#    # Build a feedforward neural network\n",
    "#    for size in sizes[:-1]:\n",
    "#        x = layers.Dense(units=size, activation=activation)(x)\n",
    "#    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "\n",
    "#SWITCHING TO RNN \n",
    "\n",
    "#def rnn_lstm(x, sizes, activation=keras.activations.tanh, output_activation=None):\n",
    "#    #Build a recurrent neural network\n",
    "#    print(x)\n",
    "#    print(sizes)\n",
    "#    for size in sizes[:-1]:\n",
    "#        #x = layers.LSTM(units=size, activation=activation)(x)\n",
    "#        x = layers.LSTM(units=size, 64, 64, activation=activation)(x)\n",
    "#    #return layers.LSTM(units=sizes[-1], activation=output_activation)(x)\n",
    "#    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "#from example online\n",
    "#model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\n",
    "\n",
    "\n",
    "\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = keras.ops.log_softmax(logits)\n",
    "    print(f\"logprobabilities uses logprobabilities_all = {logprobabilities_all}\")\n",
    "    print(f\"ops.one_hot = {keras.ops.one_hot(a, num_actions)}\")\n",
    "    logprobability = keras.ops.sum(\n",
    "        keras.ops.one_hot(a, num_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "seed_generator = keras.random.SeedGenerator(1337)\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "    print(f\"sample action calls actor with observation: {observation}\")\n",
    "    action = keras.ops.squeeze(\n",
    "        keras.random.categorical(logits, 1, seed=seed_generator), axis=1\n",
    "    )\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "):\n",
    "    print(f\"train_policy function is called with observation_buffer = {observation_buffer}\")\n",
    "    print(f\"train_policy function is called with action_buffer = {action_buffer}\")\n",
    "    print(f\"train_policy function is called with logprobability_buffer = {logprobability_buffer}\")\n",
    "\n",
    "    #force it to work. \n",
    "    \n",
    "    print(f\"force it to work\")\n",
    "    \n",
    "    #buffer_pointer = np.array([[1, 1, 1, 1]])\n",
    "    #buffer_pointer = np.repeat(buffer_pointer, 20)\n",
    "    \n",
    "    buffer_pointer = tf.constant([1, 1, 1, 1], dtype=tf.float32)\n",
    "    print(f\"buffer_pointer = {buffer_pointer}\")\n",
    "    buffer_pointer = tf.repeat(buffer_pointer, 20, axis=0)\n",
    "    print(f\"buffer_pointer after repeat = {buffer_pointer}\")\n",
    "    buffer_pointer = tf.reshape(buffer_pointer, (20, 4))\n",
    "    print(f\"buffer_pointer after reshape = {buffer_pointer}\")\n",
    "\n",
    "    observation_buffer = [[buffer_pointer,observation_buffer]]\n",
    "    print(f\"observation_buffer after trying to add buffer_pointer is {observation_buffer}\")\n",
    "\n",
    "    observation_buffer = tf.reshape(observation_buffer, (2, 20, 4))\n",
    "    print(f\"observation_buffer after reshape is {observation_buffer}\")\n",
    "    #observation_buffer = np.array([[buffer_pointer, observation_buffer.numpy()]])\n",
    "    #observation_buffer = tf.convert_to_tensor(observation_buffer)\n",
    "    \n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = keras.ops.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = keras.ops.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -keras.ops.mean(\n",
    "            keras.ops.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = keras.ops.mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = keras.ops.sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = keras.ops.mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc896e2-3a73-439d-8872-bee29c7b296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the PPO algorithm\n",
    "steps_per_epoch = 20\n",
    "epochs = 30\n",
    "gamma = 0.99\n",
    "clip_ratio = 0.2\n",
    "policy_learning_rate = 3e-4\n",
    "value_function_learning_rate = 1e-3\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "lam = 0.97\n",
    "target_kl = 0.01\n",
    "hidden_sizes = (64, 64)\n",
    "\n",
    "buffer_pointer = np.array([1, 1, 1, 1])\n",
    "# True if you want to render the environment\n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca34628-67df-4110-b375-4a9f7eb8aa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_actions is 2\n",
      "observation is [ 0.03998468 -0.00429966  0.01501964 -0.01329011]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment and get the dimensionality of the\n",
    "# observation space and the number of possible actions\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "\n",
    "#print(env.observation_space.shape[0])\n",
    "#print(env.observation_space.shape)\n",
    "\n",
    "#env.observation_space = gym.spaces.utils.flatten_space(env.observation_space.shape)\n",
    "\n",
    "\n",
    "\n",
    "#print(env.observation_space.shape[0])\n",
    "#print(env.observation_space.shape)\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "print(f\"num_actions is {num_actions}\")\n",
    "\n",
    "# Initialize the buffer\n",
    "buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
    "\n",
    "#print(f\"buffer is {buffer}\")\n",
    "\n",
    "#env.observation_space.flatten()\n",
    "\n",
    "# Initialize the actor and the critic as keras models\n",
    "#observation_input = keras.Input(shape=(observation_dimensions,None,None), dtype=\"float32\")\n",
    "\n",
    "#technically what this would mean is that you have a 4 elements in the tensor, each with 1 element\n",
    "observation_input = keras.Input(shape=(None,observation_dimensions), dtype=\"float32\")\n",
    "\n",
    "\n",
    "#print(f\"observation_input is {observation_input}\")\n",
    "#observation_input2 = keras.Input(shape=(3, 1, 1), dtype=\"float32\")\n",
    "#print(f\"observation_input2 is {observation_input2}\")\n",
    "#inputs are gonna need to change\n",
    "#logits = rnn_lstm(observation_input, list(hidden_sizes) + [num_actions])\n",
    "#actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "#value = keras.ops.squeeze(rnn_lstm(observation_input, list(hidden_sizes) + [1]), axis=1)\n",
    "#critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "#observation_input = keras.Input(shape=(4,4),  dtype=\"float32\")\n",
    "x = layers.LSTM(64, return_sequences=True, activation=\"relu\")\n",
    "#print(x)\n",
    "x = x(observation_input)\n",
    "#print(x)\n",
    "#print(x.shape)\n",
    "x = layers.LSTM(64)(x)\n",
    "logits = layers.Dense(2)(x)\n",
    "\n",
    "#logits = mlp(observation_input, list(hidden_sizes) + [num_actions])\n",
    "\n",
    "#print(f\"logits is {logits}\")\n",
    "\n",
    "\n",
    "actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "\n",
    "#actor.summary()\n",
    "\n",
    "#print(f\"actor is {actor}\")\n",
    "\n",
    "\n",
    "observation_input = keras.Input(shape=(2,4),  dtype=\"float32\")\n",
    "y = layers.LSTM(64, return_sequences=True, activation=\"relu\")\n",
    "#print(y)\n",
    "y = y(observation_input)\n",
    "#print(y)\n",
    "#print(y.shape)\n",
    "y = layers.LSTM(64)(y)\n",
    "val = layers.Dense(1)(y)\n",
    "\n",
    "#value = keras.ops.squeeze(mlp(observation_input, list(hidden_sizes) + [1]), axis=1)\n",
    "value = keras.ops.squeeze(val, axis=1)\n",
    "#print(f\"value is {value}\")\n",
    "\n",
    "critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "\n",
    "\n",
    "#print(f\"critic is {critic}\")\n",
    "\n",
    "# Initialize the policy and the value function optimizers\n",
    "policy_optimizer = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
    "value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, _ = env.reset()\n",
    "print(f\"observation is {observation}\")\n",
    "episode_return, episode_length = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b109554-8ec5-4823-8af0-185949535d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "observation.shape before adding buffer_pointer = (4,)\n",
      "observation.shape after combining with buffer_pointer = (1, 2, 4)\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.03998468 -0.00429966  0.01501964 -0.01329011]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.03998468 -0.00429966  0.01501964 -0.01329011]]]\n",
      "sample action calls actor with observation: Tensor(\"observation:0\", shape=(1, 2, 4), dtype=float64)\n",
      "after calling env.step, observation_new = [ 0.03989868 -0.19963376  0.01475383  0.2840936 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.03998468 -0.00429966  0.01501964 -0.01329011]]]\n",
      "observation[0][1] is [ 0.03998468 -0.00429966  0.01501964 -0.01329011]\n",
      "logits is [[0.01632525 0.00579936]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6878981 -0.698424 ]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.03989868 -0.19963376  0.01475383  0.28409359]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.03989868 -0.19963376  0.01475383  0.28409359]]]\n",
      "after calling env.step, observation_new = [ 0.03590601 -0.00472531  0.02043571 -0.00389979]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.03989868 -0.19963376  0.01475383  0.28409359]]]\n",
      "observation[0][1] is [ 0.03989868 -0.19963376  0.01475383  0.28409359]\n",
      "logits is [[0.0155613  0.00569561]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6882265 -0.6980922]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.03590601 -0.00472531  0.02043571 -0.00389979]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.03590601 -0.00472531  0.02043571 -0.00389979]]]\n",
      "after calling env.step, observation_new = [ 0.0358115  -0.20013429  0.02035771  0.29516014]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.03590601 -0.00472531  0.02043571 -0.00389979]]]\n",
      "observation[0][1] is [ 0.03590601 -0.00472531  0.02043571 -0.00389979]\n",
      "logits is [[0.01629407 0.00571167]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.68786997 -0.69845235]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.0358115  -0.20013429  0.02035771  0.29516014]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.0358115  -0.20013429  0.02035771  0.29516014]]]\n",
      "after calling env.step, observation_new = [ 0.03180882 -0.39554045  0.02626091  0.59419346]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.0358115  -0.20013429  0.02035771  0.29516014]]]\n",
      "observation[0][1] is [ 0.0358115  -0.20013429  0.02035771  0.29516014]\n",
      "logits is [[0.01557129 0.00570356]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.68822545 -0.6980932 ]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.03180882 -0.39554045  0.02626091  0.59419346]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.03180882 -0.39554045  0.02626091  0.59419346]]]\n",
      "after calling env.step, observation_new = [ 0.02389801 -0.20079574  0.03814478  0.3098969 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.03180882 -0.39554045  0.02626091  0.59419346]]]\n",
      "observation[0][1] is [ 0.03180882 -0.39554045  0.02626091  0.59419346]\n",
      "logits is [[0.01496422 0.00663115]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.68898934 -0.6973224 ]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.02389801 -0.20079574  0.03814478  0.30989689]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.02389801 -0.20079574  0.03814478  0.30989689]]]\n",
      "after calling env.step, observation_new = [ 0.01988209 -0.39643982  0.04434272  0.6143614 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.02389801 -0.20079574  0.03814478  0.30989689]]]\n",
      "observation[0][1] is [ 0.02389801 -0.20079574  0.03814478  0.30989689]\n",
      "logits is [[0.01559966 0.00571673]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6882179 -0.6981008]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01988209 -0.39643982  0.04434272  0.61436141]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01988209 -0.39643982  0.04434272  0.61436141]]]\n",
      "after calling env.step, observation_new = [ 0.0119533  -0.20196462  0.05662994  0.3359678 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01988209 -0.39643982  0.04434272  0.61436141]]]\n",
      "observation[0][1] is [ 0.01988209 -0.39643982  0.04434272  0.61436141]\n",
      "logits is [[0.01498721 0.00659321]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.68895894 -0.69735295]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.0119533  -0.20196462  0.05662994  0.33596781]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.0119533  -0.20196462  0.05662994  0.33596781]]]\n",
      "after calling env.step, observation_new = [ 0.007914   -0.39784482  0.0633493   0.6459574 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.0119533  -0.20196462  0.05662994  0.33596781]]]\n",
      "observation[0][1] is [ 0.0119533  -0.20196462  0.05662994  0.33596781]\n",
      "logits is [[0.01562546 0.00574697]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.68822014 -0.6980986 ]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.007914   -0.39784482  0.0633493   0.64595741]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.007914   -0.39784482  0.0633493   0.64595741]]]\n",
      "after calling env.step, observation_new = [-4.2893516e-05 -2.0366015e-01  7.6268449e-02  3.7387693e-01]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.007914   -0.39784482  0.0633493   0.64595741]]]\n",
      "observation[0][1] is [ 0.007914   -0.39784482  0.0633493   0.64595741]\n",
      "logits is [[0.01502323 0.00656296]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.68892604 -0.6973863 ]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [-4.28935164e-05 -2.03660145e-01  7.62684494e-02  3.73876929e-01]]]\n",
      "observation is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [-4.28935164e-05 -2.03660145e-01  7.62684494e-02  3.73876929e-01]]]\n",
      "after calling env.step, observation_new = [-0.0041161  -0.00969977  0.08374599  0.10618331]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [-4.28935164e-05 -2.03660145e-01  7.62684494e-02  3.73876929e-01]]]\n",
      "observation[0][1] is [-4.28935164e-05 -2.03660145e-01  7.62684494e-02  3.73876929e-01]\n",
      "logits is [[0.01561993 0.00581529]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.68825686 -0.6980615 ]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.0041161  -0.00969977  0.08374599  0.10618331]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.0041161  -0.00969977  0.08374599  0.10618331]]]\n",
      "after calling env.step, observation_new = [-0.00431009  0.18412843  0.08586966 -0.1589475 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.0041161  -0.00969977  0.08374599  0.10618331]]]\n",
      "observation[0][1] is [-0.0041161  -0.00969977  0.08374599  0.10618331]\n",
      "logits is [[0.01596841 0.00552927]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.68794125 -0.6983804 ]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.00431009  0.18412843  0.08586966 -0.1589475 ]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.00431009  0.18412843  0.08586966 -0.1589475 ]]]\n",
      "after calling env.step, observation_new = [-0.00062752 -0.01211123  0.08269071  0.1595421 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.00431009  0.18412843  0.08586966 -0.1589475 ]]]\n",
      "observation[0][1] is [-0.00431009  0.18412843  0.08586966 -0.1589475 ]\n",
      "logits is [[0.01638416 0.00709867]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6885152 -0.6978007]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "observation_new (the newer one) is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [-6.27523114e-04 -1.21112345e-02  8.26907083e-02  1.59542099e-01]]]\n",
      "observation is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [-6.27523114e-04 -1.21112345e-02  8.26907083e-02  1.59542099e-01]]]\n",
      "after calling env.step, observation_new = [-0.00086975  0.1817355   0.08588155 -0.10595108]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [-6.27523114e-04 -1.21112345e-02  8.26907083e-02  1.59542099e-01]]]\n",
      "observation[0][1] is [-0.00062752 -0.01211123  0.08269071  0.1595421 ]\n",
      "logits is [[0.01598326 0.00547235]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.68790555 -0.6984165 ]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [-8.69747775e-04  1.81735501e-01  8.58815461e-02 -1.05951078e-01]]]\n",
      "observation is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [-8.69747775e-04  1.81735501e-01  8.58815461e-02 -1.05951078e-01]]]\n",
      "after calling env.step, observation_new = [ 0.00276496 -0.01450549  0.08376253  0.21254367]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [-8.69747775e-04  1.81735501e-01  8.58815461e-02 -1.05951078e-01]]]\n",
      "observation[0][1] is [-0.00086975  0.1817355   0.08588155 -0.10595108]\n",
      "logits is [[0.01658571 0.00689388]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.688313   -0.69800484]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00276496 -0.01450549  0.08376253  0.21254367]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00276496 -0.01450549  0.08376253  0.21254367]]]\n",
      "after calling env.step, observation_new = [ 0.00247485  0.17932521  0.0880134  -0.05258562]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00276496 -0.01450549  0.08376253  0.21254367]]]\n",
      "observation[0][1] is [ 0.00276496 -0.01450549  0.08376253  0.21254367]\n",
      "logits is [[0.01600697 0.00549142]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6879033  -0.69841886]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00247485  0.17932521  0.0880134  -0.05258562]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00247485  0.17932521  0.0880134  -0.05258562]]]\n",
      "after calling env.step, observation_new = [ 0.00606136  0.37308213  0.08696169 -0.31625348]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00247485  0.17932521  0.0880134  -0.05258562]]]\n",
      "observation[0][1] is [ 0.00247485  0.17932521  0.0880134  -0.05258562]\n",
      "logits is [[0.01681989 0.00668929]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.68809474 -0.6982253 ]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00606136  0.37308213  0.08696169 -0.31625348]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00606136  0.37308213  0.08696169 -0.31625348]]]\n",
      "after calling env.step, observation_new = [ 0.013523    0.5668647   0.08063662 -0.5802945 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00606136  0.37308213  0.08696169 -0.31625348]]]\n",
      "observation[0][1] is [ 0.00606136  0.37308213  0.08696169 -0.31625348]\n",
      "logits is [[0.01634023 0.00880501]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.68938667 -0.6969219 ]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.013523    0.56686473  0.08063662 -0.58029449]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.013523    0.56686473  0.08063662 -0.58029449]]]\n",
      "after calling env.step, observation_new = [ 0.02486029  0.7607696   0.06903072 -0.84652525]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.013523    0.56686473  0.08063662 -0.58029449]]]\n",
      "observation[0][1] is [ 0.013523    0.56686473  0.08063662 -0.58029449]\n",
      "logits is [[0.01593079 0.01100588]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6906877 -0.6956126]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.02486029  0.76076961  0.06903072 -0.84652525]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.02486029  0.76076961  0.06903072 -0.84652525]]]\n",
      "after calling env.step, observation_new = [ 0.04007569  0.95488536  0.05210022 -1.1167265 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.02486029  0.76076961  0.06903072 -0.84652525]]]\n",
      "observation[0][1] is [ 0.02486029  0.76076961  0.06903072 -0.84652525]\n",
      "logits is [[0.01564733 0.01340249]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.69202536 -0.6942702 ]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.04007569  0.95488536  0.05210022 -1.11672652]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.04007569  0.95488536  0.05210022 -1.11672652]]]\n",
      "after calling env.step, observation_new = [ 0.05917339  1.1492863   0.02976569 -1.3926216 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.04007569  0.95488536  0.05210022 -1.11672652]]]\n",
      "observation[0][1] is [ 0.04007569  0.95488536  0.05210022 -1.11672652]\n",
      "logits is [[0.01532185 0.01587154]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.693422   -0.69287235]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.05917339  1.14928627  0.02976569 -1.39262164]]]\n",
      "last_value [-0.00646286]\n",
      "hitting after buffer.finish_trajectory\n",
      "observation in 'if terminal' clause is [0.00746426 0.04424315 0.03358675 0.00493572]\n",
      "made it all the way here\n",
      "train_policy function is called with observation_buffer = Tensor(\"observation_buffer:0\", shape=(20, 4), dtype=float32)\n",
      "train_policy function is called with action_buffer = Tensor(\"action_buffer:0\", shape=(20,), dtype=int32)\n",
      "train_policy function is called with logprobability_buffer = Tensor(\"logprobability_buffer:0\", shape=(20,), dtype=float32)\n",
      "force it to work\n",
      "buffer_pointer = Tensor(\"Const:0\", shape=(4,), dtype=float32)\n",
      "buffer_pointer after repeat = Tensor(\"Repeat/Reshape_1:0\", shape=(80,), dtype=float32)\n",
      "buffer_pointer after reshape = Tensor(\"Reshape:0\", shape=(20, 4), dtype=float32)\n",
      "observation_buffer after trying to add buffer_pointer is [[<tf.Tensor 'Reshape:0' shape=(20, 4) dtype=float32>, <tf.Tensor 'observation_buffer:0' shape=(20, 4) dtype=float32>]]\n",
      "observation_buffer after reshape is Tensor(\"Reshape_1:0\", shape=(2, 20, 4), dtype=float32)\n",
      "logprobabilities uses logprobabilities_all = Tensor(\"LogSoftmax:0\", shape=(2, 2), dtype=float32)\n",
      "ops.one_hot = Tensor(\"one_hot:0\", shape=(20, 2), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/var/folders/m7/4w6dht7j3h559y8h64n7qwmh0000gn/T/ipykernel_23617/3115437988.py\", line 146, in train_policy  *\n        ratio = keras.ops.exp(\n    File \"/var/folders/m7/4w6dht7j3h559y8h64n7qwmh0000gn/T/ipykernel_23617/3115437988.py\", line 94, in logprobabilities  *\n        logprobability = keras.ops.sum(\n\n    ValueError: Dimensions must be equal, but are 20 and 2 for '{{node mul}} = Mul[T=DT_FLOAT](one_hot_1, LogSoftmax)' with input shapes: [20,2], [2,2].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 104\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Update the policy and implement early stopping using KL divergence\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_policy_iterations):\n\u001b[0;32m--> 104\u001b[0m     kl \u001b[38;5;241m=\u001b[39m train_policy(\n\u001b[1;32m    105\u001b[0m         observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n\u001b[1;32m    106\u001b[0m     )\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kl \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.5\u001b[39m \u001b[38;5;241m*\u001b[39m target_kl:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;66;03m# Early Stopping\u001b[39;00m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/m7/4w6dht7j3h559y8h64n7qwmh0000gn/T/__autograph_generated_filehg2q5mh2.py:25\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_policy\u001b[0;34m(observation_buffer, action_buffer, logprobability_buffer, advantage_buffer)\u001b[0m\n\u001b[1;32m     23\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation_buffer after reshape is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mag__\u001b[38;5;241m.\u001b[39mld(observation_buffer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 25\u001b[0m     ratio \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(keras)\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mexp, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(logprobabilities), (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(actor), (ag__\u001b[38;5;241m.\u001b[39mld(observation_buffer),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), ag__\u001b[38;5;241m.\u001b[39mld(action_buffer)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(logprobability_buffer),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     26\u001b[0m     min_advantage \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(keras)\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mwhere, (ag__\u001b[38;5;241m.\u001b[39mld(advantage_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(clip_ratio)) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(advantage_buffer), (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(clip_ratio)) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(advantage_buffer)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     27\u001b[0m     policy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(keras)\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mmean, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(keras)\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mminimum, (ag__\u001b[38;5;241m.\u001b[39mld(ratio) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(advantage_buffer), ag__\u001b[38;5;241m.\u001b[39mld(min_advantage)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/var/folders/m7/4w6dht7j3h559y8h64n7qwmh0000gn/T/__autograph_generated_filekalez37x.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__logprobabilities\u001b[0;34m(logits, a)\u001b[0m\n\u001b[1;32m     11\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogprobabilities uses logprobabilities_all = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mag__\u001b[38;5;241m.\u001b[39mld(logprobabilities_all)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mops.one_hot = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(keras)\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mone_hot,\u001b[38;5;250m \u001b[39m(ag__\u001b[38;5;241m.\u001b[39mld(a),\u001b[38;5;250m \u001b[39mag__\u001b[38;5;241m.\u001b[39mld(num_actions)),\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;250m \u001b[39mfscope)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m logprobability \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(keras)\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39msum, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(keras)\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mone_hot, (ag__\u001b[38;5;241m.\u001b[39mld(a), ag__\u001b[38;5;241m.\u001b[39mld(num_actions)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(logprobabilities_all),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/var/folders/m7/4w6dht7j3h559y8h64n7qwmh0000gn/T/ipykernel_23617/3115437988.py\", line 146, in train_policy  *\n        ratio = keras.ops.exp(\n    File \"/var/folders/m7/4w6dht7j3h559y8h64n7qwmh0000gn/T/ipykernel_23617/3115437988.py\", line 94, in logprobabilities  *\n        logprobability = keras.ops.sum(\n\n    ValueError: Dimensions must be equal, but are 20 and 2 for '{{node mul}} = Mul[T=DT_FLOAT](one_hot_1, LogSoftmax)' with input shapes: [20,2], [2,2].\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "    #buffer_pointer = tf.constant([1, 1, 1, 1])\n",
    "   \n",
    "   \n",
    "    #buffer_pointer = tf.constant([1])\n",
    "    print(buffer_pointer.shape)\n",
    "    #batch_size = np.array([4])\n",
    "\n",
    "    print(f\"observation.shape before adding buffer_pointer = {observation.shape}\")\n",
    "    #observation = tf.stack([buffer_pointer, observation])\n",
    "    observation = np.array([[buffer_pointer, observation]])\n",
    "\n",
    "    print(f\"observation.shape after combining with buffer_pointer = {observation.shape}\")\n",
    "    print(f\"observation is {observation}\")\n",
    "    # Iterate over the steps of each epoch\n",
    "    for t in range(steps_per_epoch):\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "\n",
    "        #observation_new = \n",
    "        # Get the logits, action, and take one step in the environment\n",
    "        #observation = observation.reshape(1, -1)\n",
    "        #print(f\"observation after reshape is {observation}\")\n",
    "        #observation = np.array([buffer_pointer, observation])\n",
    "        #observation = buffer_pointer + observation\n",
    "        \n",
    "        #observation = tf.expand_dims(observation, axis=0)\n",
    "        #observation[0] = buffer_pointer\n",
    "\n",
    "        #observation = tf.stack([buffer_pointer, observation])\n",
    "\n",
    "        \n",
    "        print(f\"observation is {observation}\")\n",
    "        logits, action = sample_action(observation)\n",
    "        observation_new, reward, done, _, _ = env.step(action[0].numpy())\n",
    "        print(f\"after calling env.step, observation_new = {observation_new}reward = {reward} and done = {done} and \")\n",
    "        #print(f\"observation_new is {observation_new}\")\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        #observation = buffer_pointer + obvservation\n",
    "\n",
    "        \n",
    "        # Get the value and log-probability of the action\n",
    "\n",
    "        #observation = observation.reshape(1, -1)\n",
    "        print(f\"observation is {observation}\")\n",
    "        print(f\"observation[0][1] is {observation[0][1]}\")\n",
    "        observation = tf.convert_to_tensor(observation)\n",
    "        #print(observation.type())\n",
    "        #print(observation.shape)\n",
    "        #observation.reshape(1, 1, 4)\n",
    "        value_t = critic(observation)\n",
    "        print(f\"logits is {logits} and action is {action}\")\n",
    "        logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "        # Store obs, act, rew, v_t, logp_pi_t\n",
    "        buffer.store(observation[0][1], action, reward, value_t, logprobability_t)\n",
    "        \n",
    "        \n",
    "        #observation =np.array([[buffer_pointer, observation_new]])\n",
    "        # Update the observation\n",
    "        #observation = observation_new\n",
    "        \n",
    "        observation_new = np.array([[buffer_pointer, observation_new]])\n",
    "\n",
    "        observation = tf.convert_to_tensor(observation_new)\n",
    "        print(f\"observation_new (the newer one) is {observation}\")\n",
    "        # Finish trajectory if reached to a terminal state\n",
    "        terminal = done\n",
    "        if terminal or (t == steps_per_epoch - 1):\n",
    "            #last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "            last_value = 0 if done else critic(observation)\n",
    "            print(f\"last_value {last_value}\")\n",
    "            buffer.finish_trajectory(last_value)\n",
    "            print(f\"hitting after buffer.finish_trajectory\")\n",
    "            sum_return += episode_return\n",
    "            sum_length += episode_length\n",
    "            num_episodes += 1\n",
    "            observation, _ = env.reset()\n",
    "            print(f\"observation in 'if terminal' clause is {observation}\")\n",
    "            observation = np.array([[buffer_pointer, observation]])\n",
    "            observation = tf.convert_to_tensor(observation)\n",
    "            episode_return, episode_length = 0, 0\n",
    "\n",
    "    # Get values from the buffer\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        logprobability_buffer,\n",
    "    ) = buffer.get()\n",
    "\n",
    "    print(\"made it all the way here\")\n",
    "    # Update the policy and implement early stopping using KL divergence\n",
    "    for _ in range(train_policy_iterations):\n",
    "        kl = train_policy(\n",
    "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "        )\n",
    "        if kl > 1.5 * target_kl:\n",
    "            # Early Stopping\n",
    "            break\n",
    "\n",
    "    # Update the value function\n",
    "    for _ in range(train_value_iterations):\n",
    "        train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "    #buffer_pointer[0] = buffer_pointer[0]+1\n",
    "    # Print mean return and length for each epoch\n",
    "    print(\n",
    "        f\" Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131336c-b94a-45ad-a508-d0b531da62da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3b530-8327-4888-8b28-a92355965975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56113c50-b858-45e3-b835-77321b4d0ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b7d19aa-f3a1-429f-ae41-81901a245da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPO with RNN - LSTM\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from gym import *\n",
    "import scipy.signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43f10fd6-a3c3-4edc-85c7-b3cf7cb638d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOW PPO Works now (Keras example)\n",
    "\n",
    "#\n",
    "#def mlp(x, sizes, activation=keras.activations.tanh, output_activation=None):\n",
    "    # Build a feedforward neural network\n",
    "#    for size in sizes[:-1]:\n",
    "#        x = layers.Dense(units=size, activation=activation)(x)\n",
    "\n",
    "#    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "#observation_input = keras.Input(shape=(observation_dimensions,), dtype=\"float32\")\n",
    "\n",
    "\n",
    "#logits = mlp(observation_input, list(wandb.config.get('hidden_sizes')) + [num_actions])\n",
    "\n",
    "    \n",
    "#actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "\n",
    "#value = keras.ops.squeeze(mlp(observation_input, list(wandb.config.get('hidden_sizes')) + [1]), axis=1)\n",
    "#critic = keras.Model(inputs=observation_input, outputs=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60285202-4a19-41f2-84e2-661e973ff2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n",
    "\n",
    "\n",
    "#def mlp(x, sizes, activation=keras.activations.tanh, output_activation=None):\n",
    "#    # Build a feedforward neural network\n",
    "#    for size in sizes[:-1]:\n",
    "#        x = layers.Dense(units=size, activation=activation)(x)\n",
    "#    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "\n",
    "#SWITCHING TO RNN \n",
    "\n",
    "#def rnn_lstm(x, sizes, activation=keras.activations.tanh, output_activation=None):\n",
    "#    #Build a recurrent neural network\n",
    "#    print(x)\n",
    "#    print(sizes)\n",
    "#    for size in sizes[:-1]:\n",
    "#        #x = layers.LSTM(units=size, activation=activation)(x)\n",
    "#        x = layers.LSTM(units=size, 64, 64, activation=activation)(x)\n",
    "#    #return layers.LSTM(units=sizes[-1], activation=output_activation)(x)\n",
    "#    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "#from example online\n",
    "#model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\n",
    "\n",
    "\n",
    "\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = keras.ops.log_softmax(logits)\n",
    "    print(f\"logprobabilities uses logprobabilities_all = {logprobabilities_all}\")\n",
    "    print(f\"ops.one_hot = {keras.ops.one_hot(a, num_actions)}\")\n",
    "\n",
    "    #force it..\n",
    "    logprobabilities_all = tf.repeat(logprobabilities_all, 10, axis=0)\n",
    "    logprobability = keras.ops.sum(\n",
    "        keras.ops.one_hot(a, num_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "seed_generator = keras.random.SeedGenerator(1337)\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "    print(f\"sample action calls actor with observation: {observation}\")\n",
    "    action = keras.ops.squeeze(\n",
    "        keras.random.categorical(logits, 1, seed=seed_generator), axis=1\n",
    "    )\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "):\n",
    "    print(f\"train_policy function is called with observation_buffer = {observation_buffer}\")\n",
    "    print(f\"train_policy function is called with action_buffer = {action_buffer}\")\n",
    "    print(f\"train_policy function is called with logprobability_buffer = {logprobability_buffer}\")\n",
    "\n",
    "    #force it to work. \n",
    "    \n",
    "    print(f\"force it to work\")\n",
    "    \n",
    "    #buffer_pointer = np.array([[1, 1, 1, 1]])\n",
    "    #buffer_pointer = np.repeat(buffer_pointer, 20)\n",
    "    \n",
    "    buffer_pointer = tf.constant([1, 1, 1, 1], dtype=tf.float32)\n",
    "    print(f\"buffer_pointer = {buffer_pointer}\")\n",
    "    buffer_pointer = tf.repeat(buffer_pointer, 20, axis=0)\n",
    "    print(f\"buffer_pointer after repeat = {buffer_pointer}\")\n",
    "    buffer_pointer = tf.reshape(buffer_pointer, (20, 4))\n",
    "    print(f\"buffer_pointer after reshape = {buffer_pointer}\")\n",
    "\n",
    "    observation_buffer = [[buffer_pointer,observation_buffer]]\n",
    "    print(f\"observation_buffer after trying to add buffer_pointer is {observation_buffer}\")\n",
    "\n",
    "    observation_buffer = tf.reshape(observation_buffer, (2, 20, 4))\n",
    "    print(f\"observation_buffer after reshape is {observation_buffer}\")\n",
    "    #observation_buffer = np.array([[buffer_pointer, observation_buffer.numpy()]])\n",
    "    #observation_buffer = tf.convert_to_tensor(observation_buffer)\n",
    "    \n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = keras.ops.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = keras.ops.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -keras.ops.mean(\n",
    "            keras.ops.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = keras.ops.mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = keras.ops.sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    print(f\"train_value_funtion has been called with observation_buffer = {observation_buffer}\")\n",
    "    print(f\"train_value_funtion has been called with return_buffer = {return_buffer}\")\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"force it to work\")\n",
    "    \n",
    "    #buffer_pointer = np.array([[1, 1, 1, 1]])\n",
    "    #buffer_pointer = np.repeat(buffer_pointer, 20)\n",
    "    \n",
    "    buffer_pointer = tf.constant([1, 1, 1, 1], dtype=tf.float32)\n",
    "    print(f\"buffer_pointer = {buffer_pointer}\")\n",
    "    buffer_pointer = tf.repeat(buffer_pointer, 20, axis=0)\n",
    "    print(f\"buffer_pointer after repeat = {buffer_pointer}\")\n",
    "    buffer_pointer = tf.reshape(buffer_pointer, (20, 4))\n",
    "    print(f\"buffer_pointer after reshape = {buffer_pointer}\")\n",
    "\n",
    "    observation_buffer = [[buffer_pointer,observation_buffer]]\n",
    "    print(f\"observation_buffer after trying to add buffer_pointer is {observation_buffer}\")\n",
    "\n",
    "    observation_buffer = tf.reshape(observation_buffer, (2, 20, 4))\n",
    "    print(f\"observation_buffer after reshape is {observation_buffer}\")\n",
    "\n",
    "    \n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = keras.ops.mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0bc896e2-3a73-439d-8872-bee29c7b296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the PPO algorithm\n",
    "steps_per_epoch = 20\n",
    "epochs = 30\n",
    "gamma = 0.99\n",
    "clip_ratio = 0.2\n",
    "policy_learning_rate = 3e-4\n",
    "value_function_learning_rate = 1e-3\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "lam = 0.97\n",
    "target_kl = 0.01\n",
    "hidden_sizes = (64, 64)\n",
    "\n",
    "buffer_pointer = np.array([1, 1, 1, 1])\n",
    "# True if you want to render the environment\n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cca34628-67df-4110-b375-4a9f7eb8aa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_actions is 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_17\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_17\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">17,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_8 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_16 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m17,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_17 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,818</span> (198.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,818\u001b[0m (198.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,818</span> (198.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,818\u001b[0m (198.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation is [ 0.02213218 -0.02903576 -0.03626896 -0.01143116]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment and get the dimensionality of the\n",
    "# observation space and the number of possible actions\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "\n",
    "#print(env.observation_space.shape[0])\n",
    "#print(env.observation_space.shape)\n",
    "\n",
    "#env.observation_space = gym.spaces.utils.flatten_space(env.observation_space.shape)\n",
    "\n",
    "\n",
    "\n",
    "#print(env.observation_space.shape[0])\n",
    "#print(env.observation_space.shape)\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "print(f\"num_actions is {num_actions}\")\n",
    "\n",
    "# Initialize the buffer\n",
    "buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
    "\n",
    "#print(f\"buffer is {buffer}\")\n",
    "\n",
    "#env.observation_space.flatten()\n",
    "\n",
    "# Initialize the actor and the critic as keras models\n",
    "#observation_input = keras.Input(shape=(observation_dimensions,None,None), dtype=\"float32\")\n",
    "\n",
    "#technically what this would mean is that you have a 4 elements in the tensor, each with 1 element\n",
    "observation_input = keras.Input(shape=(None,observation_dimensions), dtype=\"float32\")\n",
    "\n",
    "\n",
    "#print(f\"observation_input is {observation_input}\")\n",
    "#observation_input2 = keras.Input(shape=(3, 1, 1), dtype=\"float32\")\n",
    "#print(f\"observation_input2 is {observation_input2}\")\n",
    "#inputs are gonna need to change\n",
    "#logits = rnn_lstm(observation_input, list(hidden_sizes) + [num_actions])\n",
    "#actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "#value = keras.ops.squeeze(rnn_lstm(observation_input, list(hidden_sizes) + [1]), axis=1)\n",
    "#critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "#observation_input = keras.Input(shape=(4,4),  dtype=\"float32\")\n",
    "x = layers.LSTM(64, return_sequences=True, activation=\"relu\")\n",
    "#print(x)\n",
    "x = x(observation_input)\n",
    "#print(x)\n",
    "#print(x.shape)\n",
    "x = layers.LSTM(64)(x)\n",
    "#setting it to 20 below to fix error occuring during train_policy - did not work\n",
    "logits = layers.Dense(2)(x)\n",
    "\n",
    "\n",
    "#logits = mlp(observation_input, list(hidden_sizes) + [num_actions])\n",
    "\n",
    "#print(f\"logits is {logits}\")\n",
    "\n",
    "\n",
    "actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "\n",
    "actor.summary()\n",
    "\n",
    "#actor.summary()\n",
    "\n",
    "#print(f\"actor is {actor}\")\n",
    "\n",
    "#Below worked (ish??)\n",
    "#observation_input = keras.Input(shape=(2,4),  dtype=\"float32\")\n",
    "#trying now.\n",
    "observation_input = keras.Input(shape=(None,4),  dtype=\"float32\")\n",
    "y = layers.LSTM(64, return_sequences=True, activation=\"relu\")\n",
    "#print(y)\n",
    "y = y(observation_input)\n",
    "#print(y)\n",
    "#print(y.shape)\n",
    "y = layers.LSTM(64)(y)\n",
    "val = layers.Dense(1)(y)\n",
    "\n",
    "#value = keras.ops.squeeze(mlp(observation_input, list(hidden_sizes) + [1]), axis=1)\n",
    "value = keras.ops.squeeze(val, axis=1)\n",
    "#print(f\"value is {value}\")\n",
    "\n",
    "critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "\n",
    "\n",
    "#print(f\"critic is {critic}\")\n",
    "\n",
    "# Initialize the policy and the value function optimizers\n",
    "policy_optimizer = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
    "value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, _ = env.reset()\n",
    "print(f\"observation is {observation}\")\n",
    "episode_return, episode_length = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b109554-8ec5-4823-8af0-185949535d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "observation.shape before adding buffer_pointer = (4,)\n",
      "observation.shape after combining with buffer_pointer = (1, 2, 4)\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.02213218 -0.02903576 -0.03626896 -0.01143116]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.02213218 -0.02903576 -0.03626896 -0.01143116]]]\n",
      "sample action calls actor with observation: Tensor(\"observation:0\", shape=(1, 2, 4), dtype=float64)\n",
      "after calling env.step, observation_new = [ 0.02155147 -0.2236193  -0.03649758  0.26959148]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.02213218 -0.02903576 -0.03626896 -0.01143116]]]\n",
      "observation[0][1] is [ 0.02213218 -0.02903576 -0.03626896 -0.01143116]\n",
      "logits is [[0.00247041 0.02238977]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.7031565  -0.68323714]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "logprobability during the epoch is [-0.7031565 -0.7031565 -0.7031565 -0.7031565 -0.7031565 -0.7031565\n",
      " -0.7031565 -0.7031565 -0.7031565 -0.7031565]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.02155147 -0.2236193  -0.03649758  0.26959148]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.02155147 -0.2236193  -0.03649758  0.26959148]]]\n",
      "after calling env.step, observation_new = [ 0.01707908 -0.02799603 -0.03110575 -0.03437592]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.02155147 -0.2236193  -0.03649758  0.26959148]]]\n",
      "observation[0][1] is [ 0.02155147 -0.2236193  -0.03649758  0.26959148]\n",
      "logits is [[0.00038679 0.02212404]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.70407486 -0.6823376 ]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "logprobability during the epoch is [-0.6823376 -0.6823376 -0.6823376 -0.6823376 -0.6823376 -0.6823376\n",
      " -0.6823376 -0.6823376 -0.6823376 -0.6823376]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01707908 -0.02799603 -0.03110575 -0.03437592]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01707908 -0.02799603 -0.03110575 -0.03437592]]]\n",
      "after calling env.step, observation_new = [ 0.01651916 -0.22265843 -0.03179327  0.2483328 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01707908 -0.02799603 -0.03110575 -0.03437592]]]\n",
      "observation[0][1] is [ 0.01707908 -0.02799603 -0.03110575 -0.03437592]\n",
      "logits is [[0.00261594 0.02236066]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.70306826 -0.68332356]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "logprobability during the epoch is [-0.70306826 -0.70306826 -0.70306826 -0.70306826 -0.70306826 -0.70306826\n",
      " -0.70306826 -0.70306826 -0.70306826 -0.70306826]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01651916 -0.22265843 -0.03179327  0.2483328 ]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01651916 -0.22265843 -0.03179327  0.2483328 ]]]\n",
      "after calling env.step, observation_new = [ 0.01206599 -0.41731223 -0.02682661  0.5308203 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01651916 -0.22265843 -0.03179327  0.2483328 ]]]\n",
      "observation[0][1] is [ 0.01651916 -0.22265843 -0.03179327  0.2483328 ]\n",
      "logits is [[0.00048317 0.02216709]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.7040479 -0.682364 ]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "logprobability during the epoch is [-0.7040479 -0.7040479 -0.7040479 -0.7040479 -0.7040479 -0.7040479\n",
      " -0.7040479 -0.7040479 -0.7040479 -0.7040479]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01206599 -0.41731223 -0.02682661  0.53082031]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01206599 -0.41731223 -0.02682661  0.53082031]]]\n",
      "after calling env.step, observation_new = [ 0.00371975 -0.22182341 -0.0162102   0.22980651]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01206599 -0.41731223 -0.02682661  0.53082031]]]\n",
      "observation[0][1] is [ 0.01206599 -0.41731223 -0.02682661  0.53082031]\n",
      "logits is [[-0.00106547  0.02178192]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.7046361  -0.68178874]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "logprobability during the epoch is [-0.68178874 -0.68178874 -0.68178874 -0.68178874 -0.68178874 -0.68178874\n",
      " -0.68178874 -0.68178874 -0.68178874 -0.68178874]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00371975 -0.22182341 -0.0162102   0.22980651]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00371975 -0.22182341 -0.0162102   0.22980651]]]\n",
      "after calling env.step, observation_new = [-0.00071672 -0.41671002 -0.01161407  0.51733243]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00371975 -0.22182341 -0.0162102   0.22980651]]]\n",
      "observation[0][1] is [ 0.00371975 -0.22182341 -0.0162102   0.22980651]\n",
      "logits is [[0.00058323 0.02219211]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.70401   -0.6824011]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "logprobability during the epoch is [-0.70401 -0.70401 -0.70401 -0.70401 -0.70401 -0.70401 -0.70401 -0.70401\n",
      " -0.70401 -0.70401]\n",
      "observation_new (the newer one) is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [-7.16721755e-04 -4.16710019e-01 -1.16140749e-02  5.17332435e-01]]]\n",
      "observation is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [-7.16721755e-04 -4.16710019e-01 -1.16140749e-02  5.17332435e-01]]]\n",
      "after calling env.step, observation_new = [-0.00905092 -0.22142647 -0.00126743  0.2210124 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [-7.16721755e-04 -4.16710019e-01 -1.16140749e-02  5.17332435e-01]]]\n",
      "observation[0][1] is [-0.00071672 -0.41671002 -0.01161407  0.51733243]\n",
      "logits is [[-0.00097274  0.02179573]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.7045962 -0.6818277]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "logprobability during the epoch is [-0.6818277 -0.6818277 -0.6818277 -0.6818277 -0.6818277 -0.6818277\n",
      " -0.6818277 -0.6818277 -0.6818277 -0.6818277]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.00905092 -0.22142647 -0.00126743  0.2210124 ]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.00905092 -0.22142647 -0.00126743  0.2210124 ]]]\n",
      "after calling env.step, observation_new = [-0.01347945 -0.41653028  0.00315282  0.51329523]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.00905092 -0.22142647 -0.00126743  0.2210124 ]]]\n",
      "observation[0][1] is [-0.00905092 -0.22142647 -0.00126743  0.2210124 ]\n",
      "logits is [[0.0006355  0.02220962]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.7039925  -0.68241835]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "logprobability during the epoch is [-0.7039925 -0.7039925 -0.7039925 -0.7039925 -0.7039925 -0.7039925\n",
      " -0.7039925 -0.7039925 -0.7039925 -0.7039925]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01347945 -0.41653028  0.00315282  0.51329523]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01347945 -0.41653028  0.00315282  0.51329523]]]\n",
      "after calling env.step, observation_new = [-0.02181006 -0.22145288  0.01341873  0.22160754]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01347945 -0.41653028  0.00315282  0.51329523]]]\n",
      "observation[0][1] is [-0.01347945 -0.41653028  0.00315282  0.51329523]\n",
      "logits is [[-0.0008935   0.02180194]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.70455927 -0.68186384]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "logprobability during the epoch is [-0.68186384 -0.68186384 -0.68186384 -0.68186384 -0.68186384 -0.68186384\n",
      " -0.68186384 -0.68186384 -0.68186384 -0.68186384]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02181006 -0.22145288  0.01341873  0.22160754]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02181006 -0.22145288  0.01341873  0.22160754]]]\n",
      "after calling env.step, observation_new = [-0.02623911 -0.02652527  0.01785088 -0.06681256]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02181006 -0.22145288  0.01341873  0.22160754]]]\n",
      "observation[0][1] is [-0.02181006 -0.22145288  0.01341873  0.22160754]\n",
      "logits is [[0.00066926 0.02220471]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.7039729 -0.6824374]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "logprobability during the epoch is [-0.6824374 -0.6824374 -0.6824374 -0.6824374 -0.6824374 -0.6824374\n",
      " -0.6824374 -0.6824374 -0.6824374 -0.6824374]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02623911 -0.02652527  0.01785088 -0.06681256]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02623911 -0.02652527  0.01785088 -0.06681256]]]\n",
      "after calling env.step, observation_new = [-0.02676962  0.16833626  0.01651463 -0.3538104 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02623911 -0.02652527  0.01785088 -0.06681256]]]\n",
      "observation[0][1] is [-0.02623911 -0.02652527  0.01785088 -0.06681256]\n",
      "logits is [[0.00260155 0.02229431]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.70304203 -0.68334925]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "logprobability during the epoch is [-0.68334925 -0.68334925 -0.68334925 -0.68334925 -0.68334925 -0.68334925\n",
      " -0.68334925 -0.68334925 -0.68334925 -0.68334925]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02676962  0.16833626  0.01651463 -0.3538104 ]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02676962  0.16833626  0.01651463 -0.3538104 ]]]\n",
      "after calling env.step, observation_new = [-0.02340289 -0.02701658  0.00943842 -0.05596604]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02676962  0.16833626  0.01651463 -0.3538104 ]]]\n",
      "observation[0][1] is [-0.02676962  0.16833626  0.01651463 -0.3538104 ]\n",
      "logits is [[0.00278762 0.0209404 ]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.7022647  -0.68411195]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "logprobability during the epoch is [-0.7022647 -0.7022647 -0.7022647 -0.7022647 -0.7022647 -0.7022647\n",
      " -0.7022647 -0.7022647 -0.7022647 -0.7022647]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02340289 -0.02701658  0.00943842 -0.05596604]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02340289 -0.02701658  0.00943842 -0.05596604]]]\n",
      "after calling env.step, observation_new = [-0.02394323  0.16796878  0.0083191  -0.3456562 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02340289 -0.02701658  0.00943842 -0.05596604]]]\n",
      "observation[0][1] is [-0.02340289 -0.02701658  0.00943842 -0.05596604]\n",
      "logits is [[0.00254345 0.0222804 ]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.7030644  -0.68332744]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "logprobability during the epoch is [-0.68332744 -0.68332744 -0.68332744 -0.68332744 -0.68332744 -0.68332744\n",
      " -0.68332744 -0.68332744 -0.68332744 -0.68332744]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02394323  0.16796878  0.0083191  -0.34565619]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02394323  0.16796878  0.0083191  -0.34565619]]]\n",
      "after calling env.step, observation_new = [-0.02058385 -0.02727052  0.00140597 -0.0503616 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02394323  0.16796878  0.0083191  -0.34565619]]]\n",
      "observation[0][1] is [-0.02394323  0.16796878  0.0083191  -0.34565619]\n",
      "logits is [[0.00277572 0.02092562]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.7022633 -0.6841134]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "logprobability during the epoch is [-0.7022633 -0.7022633 -0.7022633 -0.7022633 -0.7022633 -0.7022633\n",
      " -0.7022633 -0.7022633 -0.7022633 -0.7022633]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02058385 -0.02727052  0.00140597 -0.0503616 ]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02058385 -0.02727052  0.00140597 -0.0503616 ]]]\n",
      "after calling env.step, observation_new = [-0.02112926  0.16783124  0.00039874 -0.34260058]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02058385 -0.02727052  0.00140597 -0.0503616 ]]]\n",
      "observation[0][1] is [-0.02058385 -0.02727052  0.00140597 -0.0503616 ]\n",
      "logits is [[0.00252476 0.02227302]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.70307004 -0.6833218 ]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "logprobability during the epoch is [-0.6833218 -0.6833218 -0.6833218 -0.6833218 -0.6833218 -0.6833218\n",
      " -0.6833218 -0.6833218 -0.6833218 -0.6833218]\n",
      "observation_new (the newer one) is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [-2.11292617e-02  1.67831242e-01  3.98741395e-04 -3.42600584e-01]]]\n",
      "observation is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [-2.11292617e-02  1.67831242e-01  3.98741395e-04 -3.42600584e-01]]]\n",
      "after calling env.step, observation_new = [-0.01777264  0.36294752 -0.00645327 -0.63515776]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [-2.11292617e-02  1.67831242e-01  3.98741395e-04 -3.42600584e-01]]]\n",
      "observation[0][1] is [-0.02112926  0.16783124  0.00039874 -0.34260058]\n",
      "logits is [[0.00275647 0.02090006]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.70226014 -0.68411654]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "logprobability during the epoch is [-0.68411654 -0.68411654 -0.68411654 -0.68411654 -0.68411654 -0.68411654\n",
      " -0.68411654 -0.68411654 -0.68411654 -0.68411654]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01777264  0.36294752 -0.00645327 -0.63515776]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01777264  0.36294752 -0.00645327 -0.63515776]]]\n",
      "after calling env.step, observation_new = [-0.01051369  0.5581589  -0.01915643 -0.92986596]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01777264  0.36294752 -0.00645327 -0.63515776]]]\n",
      "observation[0][1] is [-0.01777264  0.36294752 -0.00645327 -0.63515776]\n",
      "logits is [[0.0023583  0.02001555]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.7020148 -0.6843575]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "logprobability during the epoch is [-0.6843575 -0.6843575 -0.6843575 -0.6843575 -0.6843575 -0.6843575\n",
      " -0.6843575 -0.6843575 -0.6843575 -0.6843575]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01051369  0.55815887 -0.01915643 -0.92986596]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01051369  0.55815887 -0.01915643 -0.92986596]]]\n",
      "after calling env.step, observation_new = [ 6.4949197e-04  7.5353408e-01 -3.7753746e-02 -1.2285067e+00]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01051369  0.55815887 -0.01915643 -0.92986596]]]\n",
      "observation[0][1] is [-0.01051369  0.55815887 -0.01915643 -0.92986596]\n",
      "logits is [[0.00172052 0.01909388]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.7018716 -0.6844982]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "logprobability during the epoch is [-0.6844982 -0.6844982 -0.6844982 -0.6844982 -0.6844982 -0.6844982\n",
      " -0.6844982 -0.6844982 -0.6844982 -0.6844982]\n",
      "observation_new (the newer one) is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.49491965e-04  7.53534079e-01 -3.77537459e-02 -1.22850668e+00]]]\n",
      "observation is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.49491965e-04  7.53534079e-01 -3.77537459e-02 -1.22850668e+00]]]\n",
      "after calling env.step, observation_new = [ 0.01572017  0.949121   -0.06232388 -1.5327749 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.49491965e-04  7.53534079e-01 -3.77537459e-02 -1.22850668e+00]]]\n",
      "observation[0][1] is [ 6.49491965e-04  7.53534079e-01 -3.77537459e-02 -1.22850668e+00]\n",
      "logits is [[0.00102897 0.01813088]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.70173466 -0.6846328 ]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "logprobability during the epoch is [-0.6846328 -0.6846328 -0.6846328 -0.6846328 -0.6846328 -0.6846328\n",
      " -0.6846328 -0.6846328 -0.6846328 -0.6846328]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01572017  0.949121   -0.06232388 -1.53277493]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01572017  0.949121   -0.06232388 -1.53277493]]]\n",
      "after calling env.step, observation_new = [ 0.0347026   1.1449361  -0.09297938 -1.8442386 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01572017  0.949121   -0.06232388 -1.53277493]]]\n",
      "observation[0][1] is [ 0.01572017  0.949121   -0.06232388 -1.53277493]\n",
      "logits is [[0.00027822 0.0171183 ]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.7016027 -0.6847626]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "logprobability during the epoch is [-0.6847626 -0.6847626 -0.6847626 -0.6847626 -0.6847626 -0.6847626\n",
      " -0.6847626 -0.6847626 -0.6847626 -0.6847626]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.0347026   1.14493608 -0.09297938 -1.84423864]]]\n",
      "last_value [0.00093051]\n",
      "hitting after buffer.finish_trajectory\n",
      "observation in 'if terminal' clause is [ 0.04888549 -0.01019829 -0.01224886 -0.01175876]\n",
      "made it all the way here\n",
      "train_policy function is called with observation_buffer = Tensor(\"observation_buffer:0\", shape=(20, 4), dtype=float32)\n",
      "train_policy function is called with action_buffer = Tensor(\"action_buffer:0\", shape=(20,), dtype=int32)\n",
      "train_policy function is called with logprobability_buffer = Tensor(\"logprobability_buffer:0\", shape=(20,), dtype=float32)\n",
      "force it to work\n",
      "buffer_pointer = Tensor(\"Const:0\", shape=(4,), dtype=float32)\n",
      "buffer_pointer after repeat = Tensor(\"Repeat/Reshape_1:0\", shape=(80,), dtype=float32)\n",
      "buffer_pointer after reshape = Tensor(\"Reshape:0\", shape=(20, 4), dtype=float32)\n",
      "observation_buffer after trying to add buffer_pointer is [[<tf.Tensor 'Reshape:0' shape=(20, 4) dtype=float32>, <tf.Tensor 'observation_buffer:0' shape=(20, 4) dtype=float32>]]\n",
      "observation_buffer after reshape is Tensor(\"Reshape_1:0\", shape=(2, 20, 4), dtype=float32)\n",
      "logprobabilities uses logprobabilities_all = Tensor(\"LogSoftmax:0\", shape=(2, 2), dtype=float32)\n",
      "ops.one_hot = Tensor(\"one_hot:0\", shape=(20, 2), dtype=float32)\n",
      "logprobabilities uses logprobabilities_all = Tensor(\"LogSoftmax_1:0\", shape=(2, 2), dtype=float32)\n",
      "ops.one_hot = Tensor(\"one_hot_2:0\", shape=(20, 2), dtype=float32)\n",
      "train_policy function is called with observation_buffer = Tensor(\"observation_buffer:0\", shape=(20, 4), dtype=float32)\n",
      "train_policy function is called with action_buffer = Tensor(\"action_buffer:0\", shape=(20,), dtype=int32)\n",
      "train_policy function is called with logprobability_buffer = Tensor(\"logprobability_buffer:0\", shape=(20,), dtype=float32)\n",
      "force it to work\n",
      "buffer_pointer = Tensor(\"Const:0\", shape=(4,), dtype=float32)\n",
      "buffer_pointer after repeat = Tensor(\"Repeat/Reshape_1:0\", shape=(80,), dtype=float32)\n",
      "buffer_pointer after reshape = Tensor(\"Reshape:0\", shape=(20, 4), dtype=float32)\n",
      "observation_buffer after trying to add buffer_pointer is [[<tf.Tensor 'Reshape:0' shape=(20, 4) dtype=float32>, <tf.Tensor 'observation_buffer:0' shape=(20, 4) dtype=float32>]]\n",
      "observation_buffer after reshape is Tensor(\"Reshape_1:0\", shape=(2, 20, 4), dtype=float32)\n",
      "logprobabilities uses logprobabilities_all = Tensor(\"LogSoftmax:0\", shape=(2, 2), dtype=float32)\n",
      "ops.one_hot = Tensor(\"one_hot:0\", shape=(20, 2), dtype=float32)\n",
      "logprobabilities uses logprobabilities_all = Tensor(\"LogSoftmax_1:0\", shape=(2, 2), dtype=float32)\n",
      "ops.one_hot = Tensor(\"one_hot_2:0\", shape=(20, 2), dtype=float32)\n",
      "train_value_function is called during epoch with observation_buffer = [[ 2.2132181e-02 -2.9035756e-02 -3.6268957e-02 -1.1431156e-02]\n",
      " [ 2.1551466e-02 -2.2361930e-01 -3.6497578e-02  2.6959148e-01]\n",
      " [ 1.7079080e-02 -2.7996030e-02 -3.1105749e-02 -3.4375925e-02]\n",
      " [ 1.6519159e-02 -2.2265843e-01 -3.1793267e-02  2.4833280e-01]\n",
      " [ 1.2065991e-02 -4.1731223e-01 -2.6826611e-02  5.3082031e-01]\n",
      " [ 3.7197464e-03 -2.2182341e-01 -1.6210204e-02  2.2980651e-01]\n",
      " [-7.1672176e-04 -4.1671002e-01 -1.1614075e-02  5.1733243e-01]\n",
      " [-9.0509215e-03 -2.2142647e-01 -1.2674264e-03  2.2101240e-01]\n",
      " [-1.3479452e-02 -4.1653028e-01  3.1528214e-03  5.1329523e-01]\n",
      " [-2.1810057e-02 -2.2145288e-01  1.3418727e-02  2.2160754e-01]\n",
      " [-2.6239114e-02 -2.6525274e-02  1.7850878e-02 -6.6812560e-02]\n",
      " [-2.6769619e-02  1.6833626e-01  1.6514625e-02 -3.5381040e-01]\n",
      " [-2.3402894e-02 -2.7016578e-02  9.4384179e-03 -5.5966038e-02]\n",
      " [-2.3943227e-02  1.6796878e-01  8.3190966e-03 -3.4565619e-01]\n",
      " [-2.0583851e-02 -2.7270516e-02  1.4059733e-03 -5.0361596e-02]\n",
      " [-2.1129262e-02  1.6783124e-01  3.9874139e-04 -3.4260058e-01]\n",
      " [-1.7772635e-02  3.6294752e-01 -6.4532706e-03 -6.3515776e-01]\n",
      " [-1.0513686e-02  5.5815887e-01 -1.9156426e-02 -9.2986596e-01]\n",
      " [ 6.4949197e-04  7.5353408e-01 -3.7753746e-02 -1.2285067e+00]\n",
      " [ 1.5720174e-02  9.4912100e-01 -6.2323876e-02 -1.5327749e+00]]\n",
      "...and return_buffer = [18.210068  17.383907  16.5494    15.706465  14.855015  13.994965\n",
      " 13.126227  12.248714  11.362338  10.467008   9.562634   8.649125\n",
      "  7.7263894  6.7943325  5.852861   4.90188    3.9412928  2.9710028\n",
      "  1.990912   1.0009212]\n",
      "train_value_funtion has been called with observation_buffer = Tensor(\"observation_buffer:0\", shape=(20, 4), dtype=float32)\n",
      "train_value_funtion has been called with return_buffer = Tensor(\"return_buffer:0\", shape=(20,), dtype=float32)\n",
      "force it to work\n",
      "buffer_pointer = Tensor(\"Const:0\", shape=(4,), dtype=float32)\n",
      "buffer_pointer after repeat = Tensor(\"Repeat/Reshape_1:0\", shape=(80,), dtype=float32)\n",
      "buffer_pointer after reshape = Tensor(\"Reshape:0\", shape=(20, 4), dtype=float32)\n",
      "observation_buffer after trying to add buffer_pointer is [[<tf.Tensor 'Reshape:0' shape=(20, 4) dtype=float32>, <tf.Tensor 'observation_buffer:0' shape=(20, 4) dtype=float32>]]\n",
      "observation_buffer after reshape is Tensor(\"Reshape_1:0\", shape=(2, 20, 4), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/var/folders/m7/4w6dht7j3h559y8h64n7qwmh0000gn/T/ipykernel_32427/1555618813.py\", line 201, in train_value_function  *\n        value_loss = keras.ops.mean((return_buffer - critic(observation_buffer)) ** 2)\n\n    ValueError: Dimensions must be equal, but are 20 and 2 for '{{node sub}} = Sub[T=DT_FLOAT](return_buffer, functional_19_1/Squeeze)' with input shapes: [20], [2].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 117\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_value_function is called during epoch with observation_buffer = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...and return_buffer = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m     train_value_function(observation_buffer, return_buffer)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#buffer_pointer[0] = buffer_pointer[0]+1\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Print mean return and length for each epoch\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Mean Return: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msum_return\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mnum_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Mean Length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msum_length\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mnum_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/m7/4w6dht7j3h559y8h64n7qwmh0000gn/T/__autograph_generated_file4z1jy_9f.py:22\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_value_function\u001b[0;34m(observation_buffer, return_buffer)\u001b[0m\n\u001b[1;32m     20\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation_buffer after reshape is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mag__\u001b[38;5;241m.\u001b[39mld(observation_buffer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 22\u001b[0m     value_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(keras)\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mmean, ((ag__\u001b[38;5;241m.\u001b[39mld(return_buffer) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(critic), (ag__\u001b[38;5;241m.\u001b[39mld(observation_buffer),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     23\u001b[0m value_grads \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(value_loss), ag__\u001b[38;5;241m.\u001b[39mld(critic)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     24\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(value_optimizer)\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(value_grads), ag__\u001b[38;5;241m.\u001b[39mld(critic)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/var/folders/m7/4w6dht7j3h559y8h64n7qwmh0000gn/T/ipykernel_32427/1555618813.py\", line 201, in train_value_function  *\n        value_loss = keras.ops.mean((return_buffer - critic(observation_buffer)) ** 2)\n\n    ValueError: Dimensions must be equal, but are 20 and 2 for '{{node sub}} = Sub[T=DT_FLOAT](return_buffer, functional_19_1/Squeeze)' with input shapes: [20], [2].\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "    #buffer_pointer = tf.constant([1, 1, 1, 1])\n",
    "   \n",
    "   \n",
    "    #buffer_pointer = tf.constant([1])\n",
    "    print(buffer_pointer.shape)\n",
    "    #batch_size = np.array([4])\n",
    "\n",
    "    print(f\"observation.shape before adding buffer_pointer = {observation.shape}\")\n",
    "    #observation = tf.stack([buffer_pointer, observation])\n",
    "    observation = np.array([[buffer_pointer, observation]])\n",
    "\n",
    "    print(f\"observation.shape after combining with buffer_pointer = {observation.shape}\")\n",
    "    print(f\"observation is {observation}\")\n",
    "    # Iterate over the steps of each epoch\n",
    "    for t in range(steps_per_epoch):\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "\n",
    "        #observation_new = \n",
    "        # Get the logits, action, and take one step in the environment\n",
    "        #observation = observation.reshape(1, -1)\n",
    "        #print(f\"observation after reshape is {observation}\")\n",
    "        #observation = np.array([buffer_pointer, observation])\n",
    "        #observation = buffer_pointer + observation\n",
    "        \n",
    "        #observation = tf.expand_dims(observation, axis=0)\n",
    "        #observation[0] = buffer_pointer\n",
    "\n",
    "        #observation = tf.stack([buffer_pointer, observation])\n",
    "\n",
    "        \n",
    "        print(f\"observation is {observation}\")\n",
    "        logits, action = sample_action(observation)\n",
    "        observation_new, reward, done, _, _ = env.step(action[0].numpy())\n",
    "        print(f\"after calling env.step, observation_new = {observation_new}reward = {reward} and done = {done} and \")\n",
    "        #print(f\"observation_new is {observation_new}\")\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        #observation = buffer_pointer + obvservation\n",
    "\n",
    "        \n",
    "        # Get the value and log-probability of the action\n",
    "\n",
    "        #observation = observation.reshape(1, -1)\n",
    "        print(f\"observation is {observation}\")\n",
    "        print(f\"observation[0][1] is {observation[0][1]}\")\n",
    "        observation = tf.convert_to_tensor(observation)\n",
    "        #print(observation.type())\n",
    "        #print(observation.shape)\n",
    "        #observation.reshape(1, 1, 4)\n",
    "        value_t = critic(observation)\n",
    "        print(f\"logits is {logits} and action is {action}\")\n",
    "        logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "        print(f\"logprobability during the epoch is {logprobability_t}\")\n",
    "\n",
    "        # Store obs, act, rew, v_t, logp_pi_t\n",
    "        buffer.store(observation[0][1], action, reward, value_t, logprobability_t[0])\n",
    "        \n",
    "        \n",
    "        #observation =np.array([[buffer_pointer, observation_new]])\n",
    "        # Update the observation\n",
    "        #observation = observation_new\n",
    "        \n",
    "        observation_new = np.array([[buffer_pointer, observation_new]])\n",
    "\n",
    "        observation = tf.convert_to_tensor(observation_new)\n",
    "        print(f\"observation_new (the newer one) is {observation}\")\n",
    "        # Finish trajectory if reached to a terminal state\n",
    "        terminal = done\n",
    "        if terminal or (t == steps_per_epoch - 1):\n",
    "            #last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "            last_value = 0 if done else critic(observation)\n",
    "            print(f\"last_value {last_value}\")\n",
    "            buffer.finish_trajectory(last_value)\n",
    "            print(f\"hitting after buffer.finish_trajectory\")\n",
    "            sum_return += episode_return\n",
    "            sum_length += episode_length\n",
    "            num_episodes += 1\n",
    "            observation, _ = env.reset()\n",
    "            print(f\"observation in 'if terminal' clause is {observation}\")\n",
    "            observation = np.array([[buffer_pointer, observation]])\n",
    "            observation = tf.convert_to_tensor(observation)\n",
    "            episode_return, episode_length = 0, 0\n",
    "\n",
    "    # Get values from the buffer\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        logprobability_buffer,\n",
    "    ) = buffer.get()\n",
    "\n",
    "    print(\"made it all the way here\")\n",
    "    # Update the policy and implement early stopping using KL divergence\n",
    "    for _ in range(train_policy_iterations):\n",
    "        kl = train_policy(\n",
    "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "        )\n",
    "        if kl > 1.5 * target_kl:\n",
    "            # Early Stopping\n",
    "            break\n",
    "\n",
    "    # Update the value function\n",
    "    for _ in range(train_value_iterations):\n",
    "        print(f\"train_value_function is called during epoch with observation_buffer = {observation_buffer}\")\n",
    "        print(f\"...and return_buffer = {return_buffer}\")\n",
    "        train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "    #buffer_pointer[0] = buffer_pointer[0]+1\n",
    "    # Print mean return and length for each epoch\n",
    "    print(\n",
    "        f\" Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131336c-b94a-45ad-a508-d0b531da62da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3b530-8327-4888-8b28-a92355965975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56113c50-b858-45e3-b835-77321b4d0ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

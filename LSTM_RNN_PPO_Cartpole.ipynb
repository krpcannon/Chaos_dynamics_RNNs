{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b7d19aa-f3a1-429f-ae41-81901a245da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPO with RNN - LSTM\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from gym import *\n",
    "import scipy.signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f10fd6-a3c3-4edc-85c7-b3cf7cb638d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOW PPO Works now (Keras example)\n",
    "\n",
    "#\n",
    "#def mlp(x, sizes, activation=keras.activations.tanh, output_activation=None):\n",
    "    # Build a feedforward neural network\n",
    "#    for size in sizes[:-1]:\n",
    "#        x = layers.Dense(units=size, activation=activation)(x)\n",
    "\n",
    "#    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "#observation_input = keras.Input(shape=(observation_dimensions,), dtype=\"float32\")\n",
    "\n",
    "\n",
    "#logits = mlp(observation_input, list(wandb.config.get('hidden_sizes')) + [num_actions])\n",
    "\n",
    "    \n",
    "#actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "\n",
    "#value = keras.ops.squeeze(mlp(observation_input, list(wandb.config.get('hidden_sizes')) + [1]), axis=1)\n",
    "#critic = keras.Model(inputs=observation_input, outputs=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60285202-4a19-41f2-84e2-661e973ff2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n",
    "\n",
    "\n",
    "#def mlp(x, sizes, activation=keras.activations.tanh, output_activation=None):\n",
    "#    # Build a feedforward neural network\n",
    "#    for size in sizes[:-1]:\n",
    "#        x = layers.Dense(units=size, activation=activation)(x)\n",
    "#    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "\n",
    "#SWITCHING TO RNN \n",
    "\n",
    "#def rnn_lstm(x, sizes, activation=keras.activations.tanh, output_activation=None):\n",
    "#    #Build a recurrent neural network\n",
    "#    print(x)\n",
    "#    print(sizes)\n",
    "#    for size in sizes[:-1]:\n",
    "#        #x = layers.LSTM(units=size, activation=activation)(x)\n",
    "#        x = layers.LSTM(units=size, 64, 64, activation=activation)(x)\n",
    "#    #return layers.LSTM(units=sizes[-1], activation=output_activation)(x)\n",
    "#    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "#from example online\n",
    "#model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\n",
    "\n",
    "\n",
    "\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = keras.ops.log_softmax(logits)\n",
    "    print(f\"logprobabilities uses logprobabilities_all = {logprobabilities_all}\")\n",
    "    print(f\"ops.one_hot = {keras.ops.one_hot(a, num_actions)}\")\n",
    "    logprobability = keras.ops.sum(\n",
    "        keras.ops.one_hot(a, num_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "seed_generator = keras.random.SeedGenerator(1337)\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "    print(f\"sample action calls actor with observation: {observation}\")\n",
    "    action = keras.ops.squeeze(\n",
    "        keras.random.categorical(logits, 1, seed=seed_generator), axis=1\n",
    "    )\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = keras.ops.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = keras.ops.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -keras.ops.mean(\n",
    "            keras.ops.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = keras.ops.mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = keras.ops.sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = keras.ops.mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc896e2-3a73-439d-8872-bee29c7b296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the PPO algorithm\n",
    "steps_per_epoch = 4000\n",
    "epochs = 30\n",
    "gamma = 0.99\n",
    "clip_ratio = 0.2\n",
    "policy_learning_rate = 3e-4\n",
    "value_function_learning_rate = 1e-3\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "lam = 0.97\n",
    "target_kl = 0.01\n",
    "hidden_sizes = (64, 64)\n",
    "\n",
    "# True if you want to render the environment\n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca34628-67df-4110-b375-4a9f7eb8aa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_actions is 2\n",
      "observation is [ 0.01577359 -0.01936189  0.03465074 -0.00034461]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment and get the dimensionality of the\n",
    "# observation space and the number of possible actions\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "\n",
    "#print(env.observation_space.shape[0])\n",
    "#print(env.observation_space.shape)\n",
    "\n",
    "#env.observation_space = gym.spaces.utils.flatten_space(env.observation_space.shape)\n",
    "\n",
    "\n",
    "\n",
    "#print(env.observation_space.shape[0])\n",
    "#print(env.observation_space.shape)\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "print(f\"num_actions is {num_actions}\")\n",
    "\n",
    "# Initialize the buffer\n",
    "buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
    "\n",
    "#print(f\"buffer is {buffer}\")\n",
    "\n",
    "#env.observation_space.flatten()\n",
    "\n",
    "# Initialize the actor and the critic as keras models\n",
    "#observation_input = keras.Input(shape=(observation_dimensions,None,None), dtype=\"float32\")\n",
    "\n",
    "#technically what this would mean is that you have a 4 elements in the tensor, each with 1 element\n",
    "observation_input = keras.Input(shape=(2,observation_dimensions), dtype=\"float32\")\n",
    "\n",
    "\n",
    "#print(f\"observation_input is {observation_input}\")\n",
    "#observation_input2 = keras.Input(shape=(3, 1, 1), dtype=\"float32\")\n",
    "#print(f\"observation_input2 is {observation_input2}\")\n",
    "#inputs are gonna need to change\n",
    "#logits = rnn_lstm(observation_input, list(hidden_sizes) + [num_actions])\n",
    "#actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "#value = keras.ops.squeeze(rnn_lstm(observation_input, list(hidden_sizes) + [1]), axis=1)\n",
    "#critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "#observation_input = keras.Input(shape=(4,4),  dtype=\"float32\")\n",
    "x = layers.LSTM(64, return_sequences=True, activation=\"relu\")\n",
    "#print(x)\n",
    "x = x(observation_input)\n",
    "#print(x)\n",
    "#print(x.shape)\n",
    "x = layers.LSTM(64)(x)\n",
    "logits = layers.Dense(2)(x)\n",
    "\n",
    "#logits = mlp(observation_input, list(hidden_sizes) + [num_actions])\n",
    "\n",
    "#print(f\"logits is {logits}\")\n",
    "\n",
    "\n",
    "actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "\n",
    "#actor.summary()\n",
    "\n",
    "#print(f\"actor is {actor}\")\n",
    "\n",
    "\n",
    "observation_input = keras.Input(shape=(2,4),  dtype=\"float32\")\n",
    "y = layers.LSTM(64, return_sequences=True, activation=\"relu\")\n",
    "#print(y)\n",
    "y = y(observation_input)\n",
    "#print(y)\n",
    "#print(y.shape)\n",
    "y = layers.LSTM(64)(y)\n",
    "val = layers.Dense(1)(y)\n",
    "\n",
    "#value = keras.ops.squeeze(mlp(observation_input, list(hidden_sizes) + [1]), axis=1)\n",
    "value = keras.ops.squeeze(val, axis=1)\n",
    "#print(f\"value is {value}\")\n",
    "\n",
    "critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "\n",
    "\n",
    "#print(f\"critic is {critic}\")\n",
    "\n",
    "# Initialize the policy and the value function optimizers\n",
    "policy_optimizer = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
    "value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, _ = env.reset()\n",
    "print(f\"observation is {observation}\")\n",
    "episode_return, episode_length = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b109554-8ec5-4823-8af0-185949535d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(4,)\n",
      "(1, 2, 4)\n",
      "observation is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.57735869e-02 -1.93618909e-02  3.46507393e-02 -3.44605680e-04]]]\n",
      "observation is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.57735869e-02 -1.93618909e-02  3.46507393e-02 -3.44605680e-04]]]\n",
      "sample action calls actor with observation: Tensor(\"observation:0\", shape=(1, 2, 4), dtype=float64)\n",
      "observation is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.57735869e-02 -1.93618909e-02  3.46507393e-02 -3.44605680e-04]]]\n",
      "observation[0][1] is [ 0.01577359 -0.01936189  0.03465074 -0.00034461]\n",
      "logits is [[-0.01195843 -0.00201394]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6981318 -0.6881873]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "observation (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01538635 -0.21496321  0.03464385  0.30306652]]]\n",
      "(1, 2, 4)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1, 2) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(observation\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#observation = tf.stack([buffer_pointer, observation])\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m observation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[buffer_pointer, observation]])\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(observation\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1, 2) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Iterate over the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "    #buffer_pointer = tf.constant([1, 1, 1, 1])\n",
    "   \n",
    "    buffer_pointer = np.array([1, 1, 1, 1])\n",
    "   \n",
    "    #buffer_pointer = tf.constant([1])\n",
    "    print(buffer_pointer.shape)\n",
    "    #batch_size = np.array([4])\n",
    "\n",
    "    # Iterate over the steps of each epoch\n",
    "    for t in range(steps_per_epoch):\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        print(observation.shape)\n",
    "        #observation = tf.stack([buffer_pointer, observation])\n",
    "        observation = np.array([[buffer_pointer, observation]])\n",
    "\n",
    "        print(observation.shape)\n",
    "        print(f\"observation is {observation}\")\n",
    "        #observation_new = \n",
    "        # Get the logits, action, and take one step in the environment\n",
    "        #observation = observation.reshape(1, -1)\n",
    "        #print(f\"observation after reshape is {observation}\")\n",
    "        #observation = np.array([buffer_pointer, observation])\n",
    "        #observation = buffer_pointer + observation\n",
    "        \n",
    "        #observation = tf.expand_dims(observation, axis=0)\n",
    "        #observation[0] = buffer_pointer\n",
    "\n",
    "        #observation = tf.stack([buffer_pointer, observation])\n",
    "\n",
    "        \n",
    "        print(f\"observation is {observation}\")\n",
    "        logits, action = sample_action(observation)\n",
    "        observation_new, reward, done, _, _ = env.step(action[0].numpy())\n",
    "        #print(f\"observation_new is {observation_new}\")\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        #observation = buffer_pointer + obvservation\n",
    "\n",
    "        \n",
    "        # Get the value and log-probability of the action\n",
    "\n",
    "        #observation = observation.reshape(1, -1)\n",
    "        print(f\"observation is {observation}\")\n",
    "        print(f\"observation[0][1] is {observation[0][1]}\")\n",
    "        observation = tf.convert_to_tensor(observation)\n",
    "        #print(observation.type())\n",
    "        #print(observation.shape)\n",
    "        #observation.reshape(1, 1, 4)\n",
    "        value_t = critic(observation)\n",
    "        print(f\"logits is {logits} and action is {action}\")\n",
    "        logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "        # Store obs, act, rew, v_t, logp_pi_t\n",
    "        buffer.store(observation[0][1], action, reward, value_t, logprobability_t)\n",
    "        \n",
    "        \n",
    "        #observation =np.array([[buffer_pointer, observation_new]])\n",
    "        # Update the observation\n",
    "        observation = observation_new\n",
    "\n",
    "        observation = np.array([[buffer_pointer, observation]])\n",
    "\n",
    "        observation = tf.convert_to_tensor(observation)\n",
    "        print(f\"observation (the newer one) is {observation}\")\n",
    "        # Finish trajectory if reached to a terminal state\n",
    "        terminal = done\n",
    "        if terminal or (t == steps_per_epoch - 1):\n",
    "            #last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "            last_value = 0 if done else critic(observation)\n",
    "            print(f\"last_value {last_value}\")\n",
    "            buffer.finish_trajectory(last_value)\n",
    "            print(f\"hitting after buffer.finish_trajectory\")\n",
    "            sum_return += episode_return\n",
    "            sum_length += episode_length\n",
    "            num_episodes += 1\n",
    "            observation, _ = env.reset()\n",
    "            print(f\"observation in 'if terminal' clause is {observation}\")\n",
    "            episode_return, episode_length = 0, 0\n",
    "\n",
    "    # Get values from the buffer\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        logprobability_buffer,\n",
    "    ) = buffer.get()\n",
    "\n",
    "    # Update the policy and implement early stopping using KL divergence\n",
    "    for _ in range(train_policy_iterations):\n",
    "        kl = train_policy(\n",
    "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "        )\n",
    "        if kl > 1.5 * target_kl:\n",
    "            # Early Stopping\n",
    "            break\n",
    "\n",
    "    # Update the value function\n",
    "    for _ in range(train_value_iterations):\n",
    "        train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "    #buffer_pointer[0] = buffer_pointer[0]+1\n",
    "    # Print mean return and length for each epoch\n",
    "    print(\n",
    "        f\" Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131336c-b94a-45ad-a508-d0b531da62da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3b530-8327-4888-8b28-a92355965975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56113c50-b858-45e3-b835-77321b4d0ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b7d19aa-f3a1-429f-ae41-81901a245da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPO with RNN - LSTM\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from gym import *\n",
    "import scipy.signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f10fd6-a3c3-4edc-85c7-b3cf7cb638d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOW PPO Works now (Keras example)\n",
    "\n",
    "#\n",
    "#def mlp(x, sizes, activation=keras.activations.tanh, output_activation=None):\n",
    "    # Build a feedforward neural network\n",
    "#    for size in sizes[:-1]:\n",
    "#        x = layers.Dense(units=size, activation=activation)(x)\n",
    "\n",
    "#    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "#observation_input = keras.Input(shape=(observation_dimensions,), dtype=\"float32\")\n",
    "\n",
    "\n",
    "#logits = mlp(observation_input, list(wandb.config.get('hidden_sizes')) + [num_actions])\n",
    "\n",
    "    \n",
    "#actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "\n",
    "#value = keras.ops.squeeze(mlp(observation_input, list(wandb.config.get('hidden_sizes')) + [1]), axis=1)\n",
    "#critic = keras.Model(inputs=observation_input, outputs=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60285202-4a19-41f2-84e2-661e973ff2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n",
    "\n",
    "\n",
    "#def mlp(x, sizes, activation=keras.activations.tanh, output_activation=None):\n",
    "#    # Build a feedforward neural network\n",
    "#    for size in sizes[:-1]:\n",
    "#        x = layers.Dense(units=size, activation=activation)(x)\n",
    "#    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "\n",
    "#SWITCHING TO RNN \n",
    "\n",
    "#def rnn_lstm(x, sizes, activation=keras.activations.tanh, output_activation=None):\n",
    "#    #Build a recurrent neural network\n",
    "#    print(x)\n",
    "#    print(sizes)\n",
    "#    for size in sizes[:-1]:\n",
    "#        #x = layers.LSTM(units=size, activation=activation)(x)\n",
    "#        x = layers.LSTM(units=size, 64, 64, activation=activation)(x)\n",
    "#    #return layers.LSTM(units=sizes[-1], activation=output_activation)(x)\n",
    "#    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "#from example online\n",
    "#model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\n",
    "\n",
    "\n",
    "\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = keras.ops.log_softmax(logits)\n",
    "    print(f\"logprobabilities uses logprobabilities_all = {logprobabilities_all}\")\n",
    "    print(f\"ops.one_hot = {keras.ops.one_hot(a, num_actions)}\")\n",
    "    logprobability = keras.ops.sum(\n",
    "        keras.ops.one_hot(a, num_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "seed_generator = keras.random.SeedGenerator(1337)\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "    print(f\"sample action calls actor with observation: {observation}\")\n",
    "    action = keras.ops.squeeze(\n",
    "        keras.random.categorical(logits, 1, seed=seed_generator), axis=1\n",
    "    )\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "):\n",
    "    print(f\"train_policy function is called with observation_buffer = {observation_buffer}\")\n",
    "    print(f\"train_policy function is called with action_buffer = {action_buffer}\")\n",
    "    print(f\"train_policy function is called with logprobability_buffer = {logprobability_buffer}\")\n",
    "\n",
    "    #force it to work. \n",
    "    \n",
    "    print(f\"force it to work\")\n",
    "    \n",
    "    #buffer_pointer = np.array([[1, 1, 1, 1]])\n",
    "    #buffer_pointer = np.repeat(buffer_pointer, 20)\n",
    "    \n",
    "    buffer_pointer = tf.constant([1, 1, 1, 1], dtype=tf.float32)\n",
    "    print(f\"buffer_pointer = {buffer_pointer}\")\n",
    "    buffer_pointer = tf.repeat(buffer_pointer, 20, axis=0)\n",
    "    print(f\"buffer_pointer after repeat = {buffer_pointer}\")\n",
    "    buffer_pointer = tf.reshape(buffer_pointer, (20, 4))\n",
    "    print(f\"buffer_pointer after reshape = {buffer_pointer}\")\n",
    "\n",
    "    observation_buffer = [[buffer_pointer,observation_buffer]]\n",
    "    print(f\"observation_buffer after trying to add buffer_pointer is {observation_buffer}\")\n",
    "\n",
    "    observation_buffer = tf.reshape(observation_buffer, (2, 20, 4))\n",
    "    print(f\"observation_buffer after reshape is {observation_buffer}\")\n",
    "    #observation_buffer = np.array([[buffer_pointer, observation_buffer.numpy()]])\n",
    "    #observation_buffer = tf.convert_to_tensor(observation_buffer)\n",
    "    \n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = keras.ops.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = keras.ops.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -keras.ops.mean(\n",
    "            keras.ops.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = keras.ops.mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = keras.ops.sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = keras.ops.mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc896e2-3a73-439d-8872-bee29c7b296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the PPO algorithm\n",
    "steps_per_epoch = 20\n",
    "epochs = 30\n",
    "gamma = 0.99\n",
    "clip_ratio = 0.2\n",
    "policy_learning_rate = 3e-4\n",
    "value_function_learning_rate = 1e-3\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "lam = 0.97\n",
    "target_kl = 0.01\n",
    "hidden_sizes = (64, 64)\n",
    "\n",
    "buffer_pointer = np.array([1, 1, 1, 1])\n",
    "# True if you want to render the environment\n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca34628-67df-4110-b375-4a9f7eb8aa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_actions is 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">17,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m17,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,818</span> (198.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,818\u001b[0m (198.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,818</span> (198.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,818\u001b[0m (198.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation is [0.01623979 0.00115319 0.0040325  0.02253819]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment and get the dimensionality of the\n",
    "# observation space and the number of possible actions\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "\n",
    "#print(env.observation_space.shape[0])\n",
    "#print(env.observation_space.shape)\n",
    "\n",
    "#env.observation_space = gym.spaces.utils.flatten_space(env.observation_space.shape)\n",
    "\n",
    "\n",
    "\n",
    "#print(env.observation_space.shape[0])\n",
    "#print(env.observation_space.shape)\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "print(f\"num_actions is {num_actions}\")\n",
    "\n",
    "# Initialize the buffer\n",
    "buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
    "\n",
    "#print(f\"buffer is {buffer}\")\n",
    "\n",
    "#env.observation_space.flatten()\n",
    "\n",
    "# Initialize the actor and the critic as keras models\n",
    "#observation_input = keras.Input(shape=(observation_dimensions,None,None), dtype=\"float32\")\n",
    "\n",
    "#technically what this would mean is that you have a 4 elements in the tensor, each with 1 element\n",
    "observation_input = keras.Input(shape=(None,observation_dimensions), dtype=\"float32\")\n",
    "\n",
    "\n",
    "#print(f\"observation_input is {observation_input}\")\n",
    "#observation_input2 = keras.Input(shape=(3, 1, 1), dtype=\"float32\")\n",
    "#print(f\"observation_input2 is {observation_input2}\")\n",
    "#inputs are gonna need to change\n",
    "#logits = rnn_lstm(observation_input, list(hidden_sizes) + [num_actions])\n",
    "#actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "#value = keras.ops.squeeze(rnn_lstm(observation_input, list(hidden_sizes) + [1]), axis=1)\n",
    "#critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "#observation_input = keras.Input(shape=(4,4),  dtype=\"float32\")\n",
    "x = layers.LSTM(64, return_sequences=True, activation=\"relu\")\n",
    "#print(x)\n",
    "x = x(observation_input)\n",
    "#print(x)\n",
    "#print(x.shape)\n",
    "x = layers.LSTM(64)(x)\n",
    "logits = layers.Dense(2)(x)\n",
    "\n",
    "#logits = mlp(observation_input, list(hidden_sizes) + [num_actions])\n",
    "\n",
    "#print(f\"logits is {logits}\")\n",
    "\n",
    "\n",
    "actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "\n",
    "actor.summary()\n",
    "\n",
    "#actor.summary()\n",
    "\n",
    "#print(f\"actor is {actor}\")\n",
    "\n",
    "\n",
    "observation_input = keras.Input(shape=(2,4),  dtype=\"float32\")\n",
    "y = layers.LSTM(64, return_sequences=True, activation=\"relu\")\n",
    "#print(y)\n",
    "y = y(observation_input)\n",
    "#print(y)\n",
    "#print(y.shape)\n",
    "y = layers.LSTM(64)(y)\n",
    "val = layers.Dense(1)(y)\n",
    "\n",
    "#value = keras.ops.squeeze(mlp(observation_input, list(hidden_sizes) + [1]), axis=1)\n",
    "value = keras.ops.squeeze(val, axis=1)\n",
    "#print(f\"value is {value}\")\n",
    "\n",
    "critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "\n",
    "\n",
    "#print(f\"critic is {critic}\")\n",
    "\n",
    "# Initialize the policy and the value function optimizers\n",
    "policy_optimizer = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
    "value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, _ = env.reset()\n",
    "print(f\"observation is {observation}\")\n",
    "episode_return, episode_length = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b109554-8ec5-4823-8af0-185949535d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "observation.shape before adding buffer_pointer = (4,)\n",
      "observation.shape after combining with buffer_pointer = (1, 2, 4)\n",
      "observation is [[[1.         1.         1.         1.        ]\n",
      "  [0.01623979 0.00115319 0.0040325  0.02253819]]]\n",
      "observation is [[[1.         1.         1.         1.        ]\n",
      "  [0.01623979 0.00115319 0.0040325  0.02253819]]]\n",
      "sample action calls actor with observation: Tensor(\"observation:0\", shape=(1, 2, 4), dtype=float64)\n",
      "after calling env.step, observation_new = [ 0.01626285 -0.19402635  0.00448326  0.31649068]reward = 1.0 and done = False and \n",
      "observation is [[[1.         1.         1.         1.        ]\n",
      "  [0.01623979 0.00115319 0.0040325  0.02253819]]]\n",
      "observation[0][1] is [0.01623979 0.00115319 0.0040325  0.02253819]\n",
      "logits is [[-0.00129499 -0.00339376]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6920983 -0.6941971]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01626285 -0.19402635  0.00448326  0.31649068]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01626285 -0.19402635  0.00448326  0.31649068]]]\n",
      "after calling env.step, observation_new = [0.01238233 0.00103145 0.01081308 0.02522498]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01626285 -0.19402635  0.00448326  0.31649068]]]\n",
      "observation[0][1] is [ 0.01626285 -0.19402635  0.00448326  0.31649068]\n",
      "logits is [[-0.0007419  -0.00430254]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6913684  -0.69492906]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[1.         1.         1.         1.        ]\n",
      "  [0.01238233 0.00103145 0.01081308 0.02522498]]]\n",
      "observation is [[[1.         1.         1.         1.        ]\n",
      "  [0.01238233 0.00103145 0.01081308 0.02522498]]]\n",
      "after calling env.step, observation_new = [ 0.01240295 -0.19424388  0.01131758  0.32129985]reward = 1.0 and done = False and \n",
      "observation is [[[1.         1.         1.         1.        ]\n",
      "  [0.01238233 0.00103145 0.01081308 0.02522498]]]\n",
      "observation[0][1] is [0.01238233 0.00103145 0.01081308 0.02522498]\n",
      "logits is [[-0.00133027 -0.00340793]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6921089  -0.69418657]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01240295 -0.19424388  0.01131758  0.32129985]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01240295 -0.19424388  0.01131758  0.32129985]]]\n",
      "after calling env.step, observation_new = [ 0.00851808 -0.38952518  0.01774357  0.6175303 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01240295 -0.19424388  0.01131758  0.32129985]]]\n",
      "observation[0][1] is [ 0.01240295 -0.19424388  0.01131758  0.32129985]\n",
      "logits is [[-0.00074269 -0.00429393]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.69137317 -0.6949244 ]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00851808 -0.38952518  0.01774357  0.61753029]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00851808 -0.38952518  0.01774357  0.61753029]]]\n",
      "after calling env.step, observation_new = [ 0.00072757 -0.19465551  0.03009418  0.33048818]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00851808 -0.38952518  0.01774357  0.61753029]]]\n",
      "observation[0][1] is [ 0.00851808 -0.38952518  0.01774357  0.61753029]\n",
      "logits is [[ 0.00017532 -0.00429365]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.69091517 -0.69538414]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [ 7.27574050e-04 -1.94655508e-01  3.00941784e-02  3.30488175e-01]]]\n",
      "observation is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [ 7.27574050e-04 -1.94655508e-01  3.00941784e-02  3.30488175e-01]]]\n",
      "after calling env.step, observation_new = [-0.00316554 -0.39019263  0.03670394  0.6325074 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [ 7.27574050e-04 -1.94655508e-01  3.00941784e-02  3.30488175e-01]]]\n",
      "observation[0][1] is [ 0.00072757 -0.19465551  0.03009418  0.33048818]\n",
      "logits is [[-0.00075195 -0.00427   ]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6913897 -0.6949077]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.00316554 -0.39019263  0.03670394  0.63250738]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.00316554 -0.39019263  0.03670394  0.63250738]]]\n",
      "after calling env.step, observation_new = [-0.01096939 -0.19560145  0.04935409  0.35160577]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.00316554 -0.39019263  0.03670394  0.63250738]]]\n",
      "observation[0][1] is [-0.00316554 -0.39019263  0.03670394  0.63250738]\n",
      "logits is [[ 0.00018643 -0.00430258]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6909052 -0.6953942]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01096939 -0.19560145  0.04935409  0.35160577]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01096939 -0.19560145  0.04935409  0.35160577]]]\n",
      "after calling env.step, observation_new = [-0.01488142 -0.39138925  0.05638621  0.6594341 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01096939 -0.19560145  0.04935409  0.35160577]]]\n",
      "observation[0][1] is [-0.01096939 -0.19560145  0.04935409  0.35160577]\n",
      "logits is [[-0.00073053 -0.00424361]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6913922 -0.6949053]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01488142 -0.39138925  0.05638621  0.65943408]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01488142 -0.39138925  0.05638621  0.65943408]]]\n",
      "after calling env.step, observation_new = [-0.0227092  -0.19709548  0.06957489  0.38502535]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01488142 -0.39138925  0.05638621  0.65943408]]]\n",
      "observation[0][1] is [-0.01488142 -0.39138925  0.05638621  0.65943408]\n",
      "logits is [[ 0.00022654 -0.00433013]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6908715  -0.69542813]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.0227092  -0.19709548  0.06957489  0.38502535]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.0227092  -0.19709548  0.06957489  0.38502535]]]\n",
      "after calling env.step, observation_new = [-0.02665111 -0.0030267   0.0772754   0.11506498]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.0227092  -0.19709548  0.06957489  0.38502535]]]\n",
      "observation[0][1] is [-0.0227092  -0.19709548  0.06957489  0.38502535]\n",
      "logits is [[-0.00067654 -0.00423449]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6913698 -0.6949277]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02665111 -0.0030267   0.0772754   0.11506498]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02665111 -0.0030267   0.0772754   0.11506498]]]\n",
      "after calling env.step, observation_new = [-0.02671165  0.19090785  0.07957669 -0.15227236]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02665111 -0.0030267   0.0772754   0.11506498]]]\n",
      "observation[0][1] is [-0.02665111 -0.0030267   0.0772754   0.11506498]\n",
      "logits is [[-0.00148172 -0.00371506]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.69203115 -0.6942645 ]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02671165  0.19090785  0.07957669 -0.15227236]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02671165  0.19090785  0.07957669 -0.15227236]]]\n",
      "after calling env.step, observation_new = [-0.02289349 -0.00525804  0.07653125  0.16441618]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02671165  0.19090785  0.07957669 -0.15227236]]]\n",
      "observation[0][1] is [-0.02671165  0.19090785  0.07957669 -0.15227236]\n",
      "logits is [[-0.00059368 -0.00304967]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6919199  -0.69437593]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02289349 -0.00525804  0.07653125  0.16441618]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02289349 -0.00525804  0.07653125  0.16441618]]]\n",
      "after calling env.step, observation_new = [-0.02299865  0.18868968  0.07981957 -0.10317563]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02289349 -0.00525804  0.07653125  0.16441618]]]\n",
      "observation[0][1] is [-0.02289349 -0.00525804  0.07653125  0.16441618]\n",
      "logits is [[-0.0014154  -0.00379097]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6919601  -0.69433564]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02299865  0.18868968  0.07981957 -0.10317563]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02299865  0.18868968  0.07981957 -0.10317563]]]\n",
      "after calling env.step, observation_new = [-0.01922486 -0.00748006  0.07775605  0.21358415]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.02299865  0.18868968  0.07981957 -0.10317563]]]\n",
      "observation[0][1] is [-0.02299865  0.18868968  0.07981957 -0.10317563]\n",
      "logits is [[-0.00079623 -0.00292211]] and action is [0]\n",
      "logprobabilities uses logprobabilities_all = [[-0.69208485 -0.6942107 ]]\n",
      "ops.one_hot = [[1. 0.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01922486 -0.00748006  0.07775605  0.21358415]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01922486 -0.00748006  0.07775605  0.21358415]]]\n",
      "after calling env.step, observation_new = [-0.01937446  0.18644908  0.08202774 -0.05359338]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01922486 -0.00748006  0.07775605  0.21358415]]]\n",
      "observation[0][1] is [-0.01922486 -0.00748006  0.07775605  0.21358415]\n",
      "logits is [[-0.00131568 -0.00390586]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6918529 -0.6944431]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01937446  0.18644908  0.08202774 -0.05359338]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01937446  0.18644908  0.08202774 -0.05359338]]]\n",
      "after calling env.step, observation_new = [-0.01564548  0.3803049   0.08095587 -0.31931028]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01937446  0.18644908  0.08202774 -0.05359338]]]\n",
      "observation[0][1] is [-0.01937446  0.18644908  0.08202774 -0.05359338]\n",
      "logits is [[-0.00092471 -0.00289834]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.69216084 -0.6941345 ]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01564548  0.3803049   0.08095587 -0.31931028]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01564548  0.3803049   0.08095587 -0.31931028]]]\n",
      "after calling env.step, observation_new = [-0.00803938  0.57418615  0.07456966 -0.58540463]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.01564548  0.3803049   0.08095587 -0.31931028]]]\n",
      "observation[0][1] is [-0.01564548  0.3803049   0.08095587 -0.31931028]\n",
      "logits is [[-0.00011421 -0.00277541]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.69181746 -0.6944787 ]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.00803938  0.57418615  0.07456966 -0.58540463]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.00803938  0.57418615  0.07456966 -0.58540463]]]\n",
      "after calling env.step, observation_new = [ 0.00344434  0.7681888   0.06286158 -0.85369664]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [-0.00803938  0.57418615  0.07456966 -0.58540463]]]\n",
      "observation[0][1] is [-0.00803938  0.57418615  0.07456966 -0.58540463]\n",
      "logits is [[ 0.00094476 -0.00254202]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.69140536 -0.6948921 ]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00344434  0.76818877  0.06286158 -0.85369664]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00344434  0.76818877  0.06286158 -0.85369664]]]\n",
      "after calling env.step, observation_new = [ 0.01880812  0.96240014  0.04578764 -1.1259692 ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.00344434  0.76818877  0.06286158 -0.85369664]]]\n",
      "observation[0][1] is [ 0.00344434  0.76818877  0.06286158 -0.85369664]\n",
      "logits is [[ 0.00196225 -0.00250652]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.6909153 -0.695384 ]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01880812  0.96240014  0.04578764 -1.12596917]]]\n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01880812  0.96240014  0.04578764 -1.12596917]]]\n",
      "after calling env.step, observation_new = [ 0.03805612  1.1568931   0.02326826 -1.403946  ]reward = 1.0 and done = False and \n",
      "observation is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.01880812  0.96240014  0.04578764 -1.12596917]]]\n",
      "observation[0][1] is [ 0.01880812  0.96240014  0.04578764 -1.12596917]\n",
      "logits is [[ 0.00295135 -0.00250543]] and action is [1]\n",
      "logprobabilities uses logprobabilities_all = [[-0.69042253 -0.69587934]]\n",
      "ops.one_hot = [[0. 1.]]\n",
      "observation_new (the newer one) is [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.03805612  1.15689313  0.02326826 -1.40394604]]]\n",
      "last_value [-0.02802336]\n",
      "hitting after buffer.finish_trajectory\n",
      "observation in 'if terminal' clause is [ 0.04856775 -0.01431023  0.01184507 -0.0214538 ]\n",
      "made it all the way here\n",
      "train_policy function is called with observation_buffer = Tensor(\"observation_buffer:0\", shape=(20, 4), dtype=float32)\n",
      "train_policy function is called with action_buffer = Tensor(\"action_buffer:0\", shape=(20,), dtype=int32)\n",
      "train_policy function is called with logprobability_buffer = Tensor(\"logprobability_buffer:0\", shape=(20,), dtype=float32)\n",
      "force it to work\n",
      "buffer_pointer = Tensor(\"Const:0\", shape=(4,), dtype=float32)\n",
      "buffer_pointer after repeat = Tensor(\"Repeat/Reshape_1:0\", shape=(80,), dtype=float32)\n",
      "buffer_pointer after reshape = Tensor(\"Reshape:0\", shape=(20, 4), dtype=float32)\n",
      "observation_buffer after trying to add buffer_pointer is [[<tf.Tensor 'Reshape:0' shape=(20, 4) dtype=float32>, <tf.Tensor 'observation_buffer:0' shape=(20, 4) dtype=float32>]]\n",
      "observation_buffer after reshape is Tensor(\"Reshape_1:0\", shape=(2, 20, 4), dtype=float32)\n",
      "logprobabilities uses logprobabilities_all = Tensor(\"LogSoftmax:0\", shape=(2, 2), dtype=float32)\n",
      "ops.one_hot = Tensor(\"one_hot:0\", shape=(20, 2), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/var/folders/m7/4w6dht7j3h559y8h64n7qwmh0000gn/T/ipykernel_31713/3115437988.py\", line 146, in train_policy  *\n        ratio = keras.ops.exp(\n    File \"/var/folders/m7/4w6dht7j3h559y8h64n7qwmh0000gn/T/ipykernel_31713/3115437988.py\", line 94, in logprobabilities  *\n        logprobability = keras.ops.sum(\n\n    ValueError: Dimensions must be equal, but are 20 and 2 for '{{node mul}} = Mul[T=DT_FLOAT](one_hot_1, LogSoftmax)' with input shapes: [20,2], [2,2].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 104\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Update the policy and implement early stopping using KL divergence\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_policy_iterations):\n\u001b[0;32m--> 104\u001b[0m     kl \u001b[38;5;241m=\u001b[39m train_policy(\n\u001b[1;32m    105\u001b[0m         observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n\u001b[1;32m    106\u001b[0m     )\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kl \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.5\u001b[39m \u001b[38;5;241m*\u001b[39m target_kl:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;66;03m# Early Stopping\u001b[39;00m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/m7/4w6dht7j3h559y8h64n7qwmh0000gn/T/__autograph_generated_filepx4866j0.py:25\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_policy\u001b[0;34m(observation_buffer, action_buffer, logprobability_buffer, advantage_buffer)\u001b[0m\n\u001b[1;32m     23\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation_buffer after reshape is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mag__\u001b[38;5;241m.\u001b[39mld(observation_buffer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 25\u001b[0m     ratio \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(keras)\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mexp, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(logprobabilities), (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(actor), (ag__\u001b[38;5;241m.\u001b[39mld(observation_buffer),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), ag__\u001b[38;5;241m.\u001b[39mld(action_buffer)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(logprobability_buffer),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     26\u001b[0m     min_advantage \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(keras)\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mwhere, (ag__\u001b[38;5;241m.\u001b[39mld(advantage_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(clip_ratio)) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(advantage_buffer), (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(clip_ratio)) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(advantage_buffer)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     27\u001b[0m     policy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(keras)\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mmean, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(keras)\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mminimum, (ag__\u001b[38;5;241m.\u001b[39mld(ratio) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(advantage_buffer), ag__\u001b[38;5;241m.\u001b[39mld(min_advantage)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/var/folders/m7/4w6dht7j3h559y8h64n7qwmh0000gn/T/__autograph_generated_filexa6ae8zm.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__logprobabilities\u001b[0;34m(logits, a)\u001b[0m\n\u001b[1;32m     11\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogprobabilities uses logprobabilities_all = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mag__\u001b[38;5;241m.\u001b[39mld(logprobabilities_all)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mops.one_hot = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(keras)\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mone_hot,\u001b[38;5;250m \u001b[39m(ag__\u001b[38;5;241m.\u001b[39mld(a),\u001b[38;5;250m \u001b[39mag__\u001b[38;5;241m.\u001b[39mld(num_actions)),\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;250m \u001b[39mfscope)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m logprobability \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(keras)\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39msum, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(keras)\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mone_hot, (ag__\u001b[38;5;241m.\u001b[39mld(a), ag__\u001b[38;5;241m.\u001b[39mld(num_actions)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(logprobabilities_all),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/var/folders/m7/4w6dht7j3h559y8h64n7qwmh0000gn/T/ipykernel_31713/3115437988.py\", line 146, in train_policy  *\n        ratio = keras.ops.exp(\n    File \"/var/folders/m7/4w6dht7j3h559y8h64n7qwmh0000gn/T/ipykernel_31713/3115437988.py\", line 94, in logprobabilities  *\n        logprobability = keras.ops.sum(\n\n    ValueError: Dimensions must be equal, but are 20 and 2 for '{{node mul}} = Mul[T=DT_FLOAT](one_hot_1, LogSoftmax)' with input shapes: [20,2], [2,2].\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "    #buffer_pointer = tf.constant([1, 1, 1, 1])\n",
    "   \n",
    "   \n",
    "    #buffer_pointer = tf.constant([1])\n",
    "    print(buffer_pointer.shape)\n",
    "    #batch_size = np.array([4])\n",
    "\n",
    "    print(f\"observation.shape before adding buffer_pointer = {observation.shape}\")\n",
    "    #observation = tf.stack([buffer_pointer, observation])\n",
    "    observation = np.array([[buffer_pointer, observation]])\n",
    "\n",
    "    print(f\"observation.shape after combining with buffer_pointer = {observation.shape}\")\n",
    "    print(f\"observation is {observation}\")\n",
    "    # Iterate over the steps of each epoch\n",
    "    for t in range(steps_per_epoch):\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "\n",
    "        #observation_new = \n",
    "        # Get the logits, action, and take one step in the environment\n",
    "        #observation = observation.reshape(1, -1)\n",
    "        #print(f\"observation after reshape is {observation}\")\n",
    "        #observation = np.array([buffer_pointer, observation])\n",
    "        #observation = buffer_pointer + observation\n",
    "        \n",
    "        #observation = tf.expand_dims(observation, axis=0)\n",
    "        #observation[0] = buffer_pointer\n",
    "\n",
    "        #observation = tf.stack([buffer_pointer, observation])\n",
    "\n",
    "        \n",
    "        print(f\"observation is {observation}\")\n",
    "        logits, action = sample_action(observation)\n",
    "        observation_new, reward, done, _, _ = env.step(action[0].numpy())\n",
    "        print(f\"after calling env.step, observation_new = {observation_new}reward = {reward} and done = {done} and \")\n",
    "        #print(f\"observation_new is {observation_new}\")\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        #observation = buffer_pointer + obvservation\n",
    "\n",
    "        \n",
    "        # Get the value and log-probability of the action\n",
    "\n",
    "        #observation = observation.reshape(1, -1)\n",
    "        print(f\"observation is {observation}\")\n",
    "        print(f\"observation[0][1] is {observation[0][1]}\")\n",
    "        observation = tf.convert_to_tensor(observation)\n",
    "        #print(observation.type())\n",
    "        #print(observation.shape)\n",
    "        #observation.reshape(1, 1, 4)\n",
    "        value_t = critic(observation)\n",
    "        print(f\"logits is {logits} and action is {action}\")\n",
    "        logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "        # Store obs, act, rew, v_t, logp_pi_t\n",
    "        buffer.store(observation[0][1], action, reward, value_t, logprobability_t)\n",
    "        \n",
    "        \n",
    "        #observation =np.array([[buffer_pointer, observation_new]])\n",
    "        # Update the observation\n",
    "        #observation = observation_new\n",
    "        \n",
    "        observation_new = np.array([[buffer_pointer, observation_new]])\n",
    "\n",
    "        observation = tf.convert_to_tensor(observation_new)\n",
    "        print(f\"observation_new (the newer one) is {observation}\")\n",
    "        # Finish trajectory if reached to a terminal state\n",
    "        terminal = done\n",
    "        if terminal or (t == steps_per_epoch - 1):\n",
    "            #last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "            last_value = 0 if done else critic(observation)\n",
    "            print(f\"last_value {last_value}\")\n",
    "            buffer.finish_trajectory(last_value)\n",
    "            print(f\"hitting after buffer.finish_trajectory\")\n",
    "            sum_return += episode_return\n",
    "            sum_length += episode_length\n",
    "            num_episodes += 1\n",
    "            observation, _ = env.reset()\n",
    "            print(f\"observation in 'if terminal' clause is {observation}\")\n",
    "            observation = np.array([[buffer_pointer, observation]])\n",
    "            observation = tf.convert_to_tensor(observation)\n",
    "            episode_return, episode_length = 0, 0\n",
    "\n",
    "    # Get values from the buffer\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        logprobability_buffer,\n",
    "    ) = buffer.get()\n",
    "\n",
    "    print(\"made it all the way here\")\n",
    "    # Update the policy and implement early stopping using KL divergence\n",
    "    for _ in range(train_policy_iterations):\n",
    "        kl = train_policy(\n",
    "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "        )\n",
    "        if kl > 1.5 * target_kl:\n",
    "            # Early Stopping\n",
    "            break\n",
    "\n",
    "    # Update the value function\n",
    "    for _ in range(train_value_iterations):\n",
    "        train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "    #buffer_pointer[0] = buffer_pointer[0]+1\n",
    "    # Print mean return and length for each epoch\n",
    "    print(\n",
    "        f\" Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131336c-b94a-45ad-a508-d0b531da62da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3b530-8327-4888-8b28-a92355965975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56113c50-b858-45e3-b835-77321b4d0ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
